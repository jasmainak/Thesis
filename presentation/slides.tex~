\documentclass{beamer}
% \documentclass[reqno]{beamer}

\usepackage{times}
\usepackage[utf8]{inputenc}  % pour les accents
\usepackage{euler}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{amsthm}
\usepackage{color}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}       % professional-quality tables

%\newtheorem{algorithm}{Algorithm}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}} 

\newcommand{\ie}{\emph{i.e.}{}}
\newcommand{\st}{\text{\emph{s.t.}}{}}
\newcommand{\eg}{\emph{e.g.}{}}
\newcommand\iid{\ensuremath{\mathit{i.i.d.}}\ }
\newcommand{\rv}{\emph{r.v.}{}}
\newcommand{\ud}{\text{d}{}}
\newcommand{\ind}{\mathds{1}{}}

\newcommand\red{\color{red} }
\newcommand\blue{\color{blue} }
\newcommand{\crit}{\mathcal{C}}



\DeclareMathOperator{\argmin}{argmin}
\def\bb{\mathbb}
\def\mb{\mathbf}
\def\leb{\text{Leb}}


% \usetheme{Darmstadt} %\theme
% \useoutertheme{sidebar}
%\useinnertheme{circles}
%\usecolortheme{whale}
\usecolortheme{seahorse}
\setbeamercolor*{frametitle}{parent=palette primary}


% \useoutertheme{infolines}
% \setbeamertemplate{navigation symbols}{} 
% \setbeamertemplate{footline}{
%   \hspace*{.5cm}\tiny{%\insertauthor
%     \hspace*{50pt}
%     \hfill\insertframenumber%/\inserttotalframenumber
%     \hspace*{.5cm}}\vspace*{.1cm}} 

%\addtobeamertemplate{footline}{\insertframenumber/\inserttotalframenumber} 
% \setbeamertemplate{footline}{\hspace*{.5cm}\scriptsize{%\insertauthor
%     \hspace*{50pt} \hfill\insertframenumber\hspace*{.5cm}}\vspace*{.1cm}} 

%\setbeamertemplate{theorems}[numbered]

%\setbeamercolor{block title}{bg=lightgray,fg=blue}

\title{
Machine Learning Methods for Anomaly Detection
}

\author[Nicolas Goix]{ Nicolas Goix \\
% \\
% \vspace{1em}
 {\tiny LTCI, CNRS, Telecom ParisTech, Université Paris-Saclay, France 
 }
}
%}
\date{PhD Defence, Telecom Paristech, October 2016}



\AtBeginSection[]
{
 \begin{frame}<beamer>
 \frametitle{}
 \tableofcontents[currentsection]
 \end{frame}
}

\begin{document}
\begin{frame}
 \titlepage
\end{frame}


% \begin{frame}
% \frametitle{Outline}
% \tableofcontents%[section, subsection]
% \end{frame}

% \begin{frame}
%   \frametitle{Outline}
%   \tableofcontents
% \end{frame}


%\section{Introduction}



\begin{frame}
\frametitle{Anomaly Detection (AD)}

\begin{alertblock}{What is Anomaly Detection ?}
`Finding patterns in the data that do not conform to expected behavior'
\end{alertblock}
\begin{figure}
\includegraphics[height=5cm]{AD_intro.png}
\end{figure}
Huge number of applications: Network intrusions, credit card fraud detection, insurance, finance, military surveillance,... \\~\\
\end{frame}



\begin{frame}
\frametitle{Machine Learning context}
%In a machine learning context, AD can been seen as a classification task but where the usual assumption -dataset contains info regarding all classes- breaks down.
\begin{block}{Different kind of Anomaly Detection}
\begin{itemize}
\item \textbf{Supervised} AD {\red (not dealt with)}\\
Labels available for both normal data and anomalies\\
(similar to rare class mining)\\~\\

\item \textbf{Novelty Detection} {\red (our theoretical framework)}\\
The algorithm learns on normal data only\\~\\

\item \textbf{Outlier Detection} {\red (extended application framework)}\\
Training set (unlabeled) = normal + abnormal data \\
(assumption: anomalies are very rare)
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Guidelines}
%In a machine learning context, AD can been seen as a classification task but where the usual assumption -dataset contains info regarding all classes- breaks down.
{\small An AD algorithm returns a {\blue scoring function $s: \mathbb{R}^d \to \mathbb{R}$}}.\\
{\footnotesize It represents the {\blue `degree of abnormality'} of an observation $x \in \mathbb{R}^d$} \\~\\

\begin{block}{}
\begin{itemize}
\item Part I: Performance criterion on {\blue $s$}.\\
{\small ~~(model selection)} \\~\\~\\
%{\small ~~~~~~~~~~~~ $\to$ Evaluation criterion for AD algorithms.}\\~\\~\\~\\
\item Part II: Building good {\blue $s$} on extreme regions. \\
{\small ~~ (model design)}
%{\small ~~~~~~~~~~~~ $\to$ Gaining in accuracy on extreme regions.}
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Part I: (unsupervised) performance criterion}

%\subsection{Goal of Anomaly Detection }
%Anomaly: "an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism (Hawkins 1980)"

\begin{block}{Such a criterion allows:}
\begin{itemize}
\item{1-} To build good {\blue $s$} by optimizing this criterion.
\item{2-} To evaluate any AD algorithm without using any labels.
\end{itemize}
\end{block}

\begin{block}{Practical motivations:}
Most of the time, data come without any label.\\
{\small ~~~~~~ $\to$ no ROC or PR curves!}
\end{block}

\begin{block}{Idea:}
\centering
How good is an anomaly detection algorithm?
$$\downarrow$$
How good is it estimating the level sets?
\end{block}
\end{frame}



\begin{frame}
\frametitle{Context}
%In a machine learning context, AD can been seen as a classification task but where the usual assumption -dataset contains info regarding all classes- breaks down.

\begin{block}{Novelty Detection { \footnotesize (One-Class Classification, semi-supervised AD)}}

\begin{itemize}
\item \textbf{Data: {\red inliers}}.\\
{\small i.i.d. observations in $\mathbb{R}^d$ from the normal behavior, density {\red $f$}.}\\
%{\footnotesize Remark: In practice, data can be polluted by a small proportion of anomalies.}

\item \textbf{Output to evaluate: {\blue scoring function.}} \\
{\small
- AD algorithms return {\blue $s: \mathbb{R}^d \to \mathbb{R}$}.\\
- $s$ defined a {\blue pre-order} on $\mathbb{R}^d$ = `degree of abnormality'.\\
- $s$ level sets are estimates of $f$ level sets.\\
- $s$ can be interpreted as a box which contains {\blue an infinite number of level sets estimates} (at different levels)}.\\
\end{itemize}

\textbf{Remark.} Perfect scoring functions: $s=f$ or $s=2f+3$ or $s=T \circ f$ any increasing transform of $f$. \\
%($f=2s$, $f=s^2$ if $s \ge 0$, ...).

\end{block}
\end{frame}


\begin{frame}
\frametitle{Problem reformulation}
How can we know % if the preorder induced by a scoring function s is ‘close’ to that of f , or equivalently
if the level sets of s are close to those of f ?
\begin{block}{Performance criterion for a scoring function.}
\begin{itemize}
\item \textbf{Fact:} For any strictly increasing transform $T$, level sets of $T \circ f$ are exactly those of $f$.\\
$\Rightarrow$ Criterion $\mathcal{C}(s) = \|s-f\|$ does'nt work! ($s = 2f$ is perfect)

\item \textbf{We would like}\\
- $\crit^{\Phi}(s) = \| \Phi(s) - \Phi(f) \|$ with $\Phi$ s.t. $\Phi(T \circ s) = \Phi(s)$. \\
- $ \{ \text{level sets of optimal }~ s^*\} = \{ \text{level sets of } ~f \}$. \\
- $\crit^{\Phi}(s)$ = `distance' between level sets of $s$ and those of $f$.

\end{itemize}
\end{block}
 $\Rightarrow \Phi(s) := MV_s$ or $EM_s$, the Mass-Volume and Excess-Mass curves of $s$.
\end{frame}


\begin{frame}
\frametitle{Criteria satisfying these requirements: MV and EM}

\begin{block}{Mass-volume and excess-mass curves}
\begin{itemize}
\item \textbf{Definitions:}
\small{
\begin{align*}
& MV_s(\alpha) = \inf_{u \ge 0}~~ \leb(s \ge u) ~~~~s.t.~~~~ \mathbb{P}(s(\mb X) \ge u) \ge \alpha\\
& EM_s(t) = \sup_{u \ge 0}~~\left\{ \mathbb{P}(s(\mb X) \ge u) ~-~ t \leb(s \ge u) \right\}
\end{align*}
}
\item \textbf{Optimal curves:}
\small{
\begin{align*}
&MV^*(\alpha)  = \min_{\Omega~ \text{borelian}} ~\leb(\Omega) ~~~\st~~ \mathbb{P}(\mb X \in \Omega) \ge \alpha &&= MV_f = MV_{T \circ f}\\
&EM^*(t)  = ~~\max_{\Omega\text{ borelian} } \{ {\mathbb{P}} (\mb X\in \Omega)-t\leb(\Omega) \} &&= EM_f = EM_{T \circ f}
\end{align*}
}
\item \textbf{Interpretation:} $(EM_s - EM_f)(t) \simeq ~\inf_{u>0}~ \leb(\{ s >u\}~\Delta~\{f>t\})$ 
\small{
\begin{align*}
&\| EM_s - EM_f\|_{L^1(I)} : \text{ distance between $t$ - level sets of $s$ and $f$, $t \in I$}.\\
&\| MV_s - MV_f\|_{L^1(J)} : \text{ ~distance between $\alpha$-level sets of $s$ and $f$, $\alpha \in J$}. \\
%&(EM_s - EM_f)(t) \simeq \inf_{u>0} Leb (\{ s >u\}\Delta\{f>t\})
\end{align*}
}
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\begin{figure}
\includegraphics[width = 0.7\linewidth]{emmv.png}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{Estimation and Issues}

\begin{itemize}
\item \textbf{Estimation:}
\small{
\begin{align*}
&\widehat{MV}_s(\alpha) = \inf_{u \ge 0} ~~~\leb(s \ge u) ~~~~\st~~~~ \mathbb{P}_n(s \ge u) \ge \alpha\\
&\widehat{EM}_s(t) = \sup_{u \ge 0} ~~~\mathbb{P}_n(s \ge u) ~-~ t \leb(s \ge u)
\end{align*}
}


\item \textbf{Empirical criteria:}
\small{
\begin{align*}
\widehat{\crit}^{EM}(s) &= \| \widehat{EM}_s \|_{L^1(I)}  &&I = [0,\widehat{EM}^{-1}(0.9)],\\
\widehat{\crit}^{MV}(s) &= \| \widehat{MV}_s \|_{L^1(J)}  &&J = [0.9, 1],
\end{align*}
}


\item \textbf{Issues:}
\small{
The volume $\leb(s \ge u)$ has to be estimated (Monte-Carlo). Challenging in large dimensions.
}

\end{itemize}
\end{frame}







\begin{frame}
\frametitle{Solution}

\begin{block}{Feature sub-sampling and Aggregating}~\\

\small{
%\begin{algorithm}
\begin{algorithmic}

  \STATE \textbf{Inputs}: AD algorithm $\mathcal{A}$, data set $X$ size $n \times d $, feature sub-sampling size $d'$, number of draws $m$.\\~\\
  \FOR{$k=1,\ldots,m$}
    \STATE{-randomly select a sub-group $F_k$ of $d'$ features}
    \STATE{-compute the associated scoring function $s_{k} = \mathcal{A}\big((x^j_i)_{1 \le i \le n,~j \in F_k}\big)$}
    \STATE -compute $\widehat{\crit}_k^{EM} = \| \widehat{EM}_{s_k} \|_{L^1(I)}$  or $\widehat{\crit}_k^{MV} = \| \widehat{MV}_{s_k} \|_{L^1(J)}$
  \ENDFOR \\~\\

  \STATE \textbf{Return} performance criteria: $$\widehat{\crit}^{EM}_{high\_dim} (\mathcal{A})= \frac{1}{m} \sum_{k=1}^m\widehat \crit_k^{EM} \text{~~~~or~~~~} \widehat{\crit}^{MV}_{high\_dim}(\mathcal{A}) = \frac{1}{m} \sum_{k=1}^m\widehat \crit_k^{MV}~.$$

\end{algorithmic}
%\end{algorithm}
}

\end{block}
\end{frame}









\begin{frame}
\frametitle{Benchmarks}
\begin{block}{Does performance in term of EM/MV correspond to performance in term of ROC/PR?}

\begin{itemize}
\item \textbf{Experiments:}
{\small
12 datasets, 3 AD algorithms (LOF, OCSVM, iForest)
$\to$ 36 possible pairwise comparisons:
\begin{align*}
\bigg\{~~\Big(A_1 \text{~on~} \mathcal{D},~ A_2 \text{~on~} \mathcal{D}\Big),~~ & A_1, A_2 \in \{\text{iForest, LOF, OCSVM}\}, \\
& \mathcal{D} \in \{\text{adult, http, \ldots, spambase}\} ~~\bigg\}.
\end{align*}
}


\item \textbf{Results:}
{ \small
If we only consider the pairs \st~\emph{ROC and PR agree on which algorithm is the best}, we are able (with EM and MV scores) to recover it in $80\%$ of the cases.
}
\end{itemize}


\end{block}
\end{frame}



% \begin{frame}
% \begin{figure}
% \includegraphics[width=\linewidth]{mv_em_adult_supervised_09_factorized_inkscape.png}
% \end{figure}
% \end{frame}


\begin{frame}
\centering
\Large{Thank you!}
\end{frame}


\begin{frame}
\begin{table}[!ht]
\caption{Original Datasets characteristics}
\centering
%\tabcolsep=0.2cm
\resizebox{\linewidth}{!} {
\begin{tabular}{l cc ll }
  \toprule
  ~           & nb of samples      & nb of features     & ~~~~~~~~~~~~~~~~~~~~~~~~~anomaly class      & ~                  \\ \cmidrule{1-5}
  adult       & 48842              & 6                  &    class '$>50K$'                           &      (23.9\%)      \\
  http        & 567498             & 3                  &      attack                                 &    (0.39\%)        \\
  pima        & 768                & 8                  &    pos (class 1)                            &        (34.9\%)    \\
  smtp        & 95156              & 3                  &      attack                                 &    (0.03\%)        \\
  wilt        & 4839               & 5                  &    class 'w' (diseased trees)               &    (5.39\%)        \\
  annthyroid  & 7200               & 6                  &    classes $\neq$ 3                         &        (7.42\%)    \\
  arrhythmia  & 452                & 164                &    classes $\neq$ 1 (features 10-14 removed)&  (45.8\%)          \\
  forestcover & 286048             & 10                 &    class 4  (vs. class 2 )                  &           (0.96\%) \\
  ionosphere  & 351                & 32                 &    bad                                      &       (35.9\%)     \\
  pendigits   & 10992              & 16                 &    class 4                                  &        (10.4\%)    \\
  shuttle     & 85849              & 9                  &      classes $\neq$ 1 (class 4 removed)     &  (7.17\%)          \\
  spambase    & 4601               & 57                 &    spam                                     &           (39.4\%) \\
  \bottomrule
\end{tabular}
}
\end{table}

\end{frame}


\begin{frame}
\begin{table}[!ht]
\centering
\footnotesize
\caption{\footnotesize Results for the novelty detection setting. One can see that ROC, PR, EM, MV often do agree on which algorithm is the best (in bold), which algorithm is the worse (underlined) on some fixed datasets. When they do not agree, it is often because ROC and PR themselves do not, meaning that the ranking is not clear.}
\tabcolsep=0.11cm
\resizebox{\linewidth}{!} {
\begin{tabular}{l cccc c cccc c cccc}
\toprule
Dataset      & \multicolumn{4}{c}{iForest}& & \multicolumn{4}{c}{OCSVM}&  & \multicolumn{4}{c}{LOF} \\ %& parameters $(\epsilon, k)$\\
  \cmidrule{1-15}

~            & ROC  & PR   & EM    &  MV  &  & ROC  & PR   & EM    & MV     &  & ROC  & PR   & EM    & MV    \\
adult        &\bf 0.661 &\bf 0.277 &\bf 1.0e-04&\bf 7.5e01&  &0.642 &0.206 &2.9e-05& 4.3e02 &  &\underline{0.618} &\underline{0.187}&\underline{1.7e-05}&\underline{9.0e02} \\
http         &0.994 &0.192 &1.3e-03&9.0   &  &\bf 0.999 &\bf 0.970 &\bf 6.0e-03&\bf 2.6  &     &\underline{0.946} &\underline{0.035} &\underline{8.0e-05}&\underline{3.9e02} \\
pima         &0.727 &0.182 &5.0e-07&\bf 1.2e04&  &\bf 0.760 &\bf 0.229 &\bf 5.2e-07&\underline{1.3e04} &   &\underline{0.705} &\underline{0.155} &\underline{3.2e-07}&2.1e04 \\
smtp         &0.907 &\underline{0.005} &\underline{1.8e-04}&\underline{9.4e01}&  &\underline{0.852} &\bf 0.522 &\bf 1.2e-03&8.2    &   &\bf 0.922 &0.189 & 1.1e-03&\bf 5.8    \\
wilt         &0.491 &0.045 &4.7e-05&\underline{2.1e03} & &\underline{0.325} &\underline{0.037} &\bf 5.9e-05&\bf 4.5e02 &   &\bf 0.698 &\bf 0.088 &\underline{2.1e-05}&1.6e03 \\ 
 &&&&&&&&&&&&&& \\
% internet\_ads&0.414 &0.109 &  NA   &  NA    &      &      &       &          &0.540 &0.139 &       &       \\
% SA           &0.988 &0.386 &  NA   &  NA    &      &      &       &          &0.880 &0.018 &       &       \\
% SF           &0.936 &0.038 &  NA   &  NA    &      &      &       &          &0.891 &0.022 &       &       \\
annthyroid   &\bf 0.913 &\bf 0.456 &\bf 2.0e-04&2.6e02 & &\underline{0.699} &\underline{0.237} &\underline{6.3e-05}&\bf 2.2e02 &   &0.823 &0.432 &6.3e-05&\underline{1.5e03} \\
arrhythmia   &\bf 0.763 &\bf 0.487 &\bf 1.6e-04&\bf 9.4e01 & &0.736 &0.449 &1.1e-04&1.0e02   & &\underline{0.730} &\underline{0.413} &\underline{8.3e-05}&\underline{1.6e02} \\
forestcov.   &\underline{0.863} &\underline{0.046} &\underline{3.9e-05}&\underline{2.0e02}&  &0.958 &0.110 &5.2e-05&1.2e02  &  &\bf 0.990 &\bf 0.792 &\bf 3.5e-04&\bf 3.9e01 \\
ionosphere   &\underline{0.902} &\underline{0.529} &\underline{9.6e-05}&\underline{7.5e01} & &\bf 0.977 &\bf 0.898 &\bf 1.3e-04&\bf 5.4e01  &  &0.971 &0.895 &1.0e-04&7.0e01 \\
pendigits    &0.811 &0.197 &2.8e-04&2.6e01 & &\underline{0.606} &\underline{0.112} &\underline{2.7e-04}&\underline{2.7e01}   & &\bf 0.983 &\bf 0.829 &\bf 4.6e-04&\bf 1.7e01 \\
shuttle      &0.996 &0.973 &1.8e-05&5.7e03 & &\underline{0.992} &\underline{0.924} &\bf 3.2e-05&\bf 2.0e01   & &\bf 0.999 &\bf 0.994 &\underline{7.9e-06}&\underline{2.0e06} \\
spambase     &\bf 0.824 &\bf 0.371 &\bf 9.5e-04&\bf 4.5e01&  &\underline{0.729} &0.230 &4.9e-04&1.1e03  &  &0.754 &\underline{0.173} &\underline{2.2e-04}&\underline{4.1e04} \\
\bottomrule
\end{tabular}
}
\end{table}

\end{frame}

\end{document}
