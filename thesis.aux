\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Contributions}{iii}{dummy.1}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{dummy.2}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xiii}{dummy.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Summary}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{1}{Summary}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\citation{Scholkopf2001,Steinwart2005,Scott2006,VertVert}
\citation{Scholkopf2001,Scott2006,VertVert}
\citation{Tsybakov1997,Cuevas1997,Baillo2001,Baillo2003,Cadre2006,Rigollet2009,Mason2009}
\@writefile{toc}{\contentsline {paragraph}{Notations}{2}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Anomaly Detection, Anomaly Ranking and Scoring Functions}{2}{section.1.2}}
\newlabel{resume:scoring_function}{{1.2}{2}{Anomaly Detection, Anomaly Ranking and Scoring Functions}{section.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Summary of notation}}{3}{table.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notations}{{1.1}{3}{Summary of notation}{table.caption.5}{}}
\citation{Liu2008}
\citation{AISTAT15}
\citation{CLEM13}
\citation{Polonik97,Einmahl1992}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}M-estimation and criteria for scoring functions}{4}{section.1.3}}
\newlabel{resume:scoring}{{1.3}{4}{M-estimation and criteria for scoring functions}{section.1.3}{}}
\citation{Polonik97}
\citation{Scott2006}
\citation{CLEM13,CLEM14}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Minimum Volume sets}{5}{subsection.1.3.1}}
\newlabel{resume:mv-set}{{1.3.1}{5}{Minimum Volume sets}{subsection.1.3.1}{}}
\newlabel{eq:MV}{{1.1}{5}{Minimum Volume sets}{equation.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Mass-Volume curve}{5}{subsection.1.3.2}}
\newlabel{resume:mv-curve}{{1.3.2}{5}{Mass-Volume curve}{subsection.1.3.2}{}}
\newlabel{eq:alpha_beta}{{1.2}{6}{Mass-Volume curve}{equation.1.3.2}{}}
\newlabel{def:MV}{{1.3}{6}{Mass-Volume curve}{equation.1.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Mass-Volume at level $\alpha $\relax }}{6}{figure.caption.6}}
\newlabel{MVcurve}{{1.1}{6}{Mass-Volume at level $\alpha $\relax }{figure.caption.6}{}}
\citation{Polonik95}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}The Excess-Mass criterion}{7}{subsection.1.3.3}}
\newlabel{resume:em-curve}{{1.3.3}{7}{The Excess-Mass criterion}{subsection.1.3.3}{}}
\newlabel{solomeg}{{1.4}{7}{The Excess-Mass criterion}{equation.1.3.4}{}}
\newlabel{EM}{{1.5}{7}{The Excess-Mass criterion}{equation.1.3.5}{}}
\newlabel{EMcurve}{{1.3.3}{7}{The Excess-Mass criterion}{equation.1.3.5}{}}
\citation{AISTAT15}
\citation{CLEM13}
\citation{COLT15}
\citation{Huangphd,Drees98,Embrechts2000,dHF06}
\citation{Qi97,Einmahl2012}
\citation{COLT15}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Accuracy on extreme regions}{9}{section.1.4}}
\newlabel{resume:extreme}{{1.4}{9}{Accuracy on extreme regions}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Extreme Values Analysis through STDF estimation}{9}{subsection.1.4.1}}
\newlabel{resume:stdf}{{1.4.1}{9}{Extreme Values Analysis through STDF estimation}{subsection.1.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries.}{9}{subsection.1.4.1}}
\citation{Einmahl2001,Einmahl2009}
\@writefile{toc}{\contentsline {paragraph}{Learning the dependence structure of rare events.}{10}{subsection.1.4.1}}
\newlabel{thm-princ}{{1.2}{10}{}{theorem.1.4.2}{}}
\newlabel{thm:l}{{1.3}{10}{}{theorem.1.4.3}{}}
\citation{AISTAT16}
\citation{ARXIV16}
\citation{Roberts99,Roberts2000,Clifton2011,Clifton2008,Lee2008}
\citation{coles1991modeling,fougeres2009models,cooley2010pairwise,sabourinNaveau2012}
\citation{stephenson2009high}
\citation{Einmahl2001,Einmahl2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Sparse Representation of Multivariate Extremes}{11}{subsection.1.4.2}}
\newlabel{resume:sec:JMVA}{{1.4.2}{11}{Sparse Representation of Multivariate Extremes}{subsection.1.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Truncated cones in 3D\relax }}{12}{figure.1.2}}
\newlabel{fig:3Dcones}{{1.2}{12}{Truncated cones in 3D\relax }{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Truncated $\epsilon $-cones in 2D\relax }}{12}{figure.1.3}}
\newlabel{2Dcones}{{1.3}{12}{Truncated $\epsilon $-cones in 2D\relax }{figure.1.3}{}}
\citation{ICMLworkshop16}
\citation{NIPS16evaluation}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Heuristic approaches}{13}{section.1.5}}
\newlabel{resume:sec:heuristic}{{1.5}{13}{Heuristic approaches}{section.1.5}{}}
\citation{Provost1997,Provost1998,Fawcett2006}
\citation{Davis2006,Clemencon2009}
\citation{Thomas2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Evaluation of anomaly detection algorithms}{14}{subsection.1.5.1}}
\newlabel{resume:evaluation}{{1.5.1}{14}{Evaluation of anomaly detection algorithms}{subsection.1.5.1}{}}
\newlabel{intro:MV-def}{{1.6}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.6}{}}
\newlabel{intro:EM-def}{{1.7}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.7}{}}
\newlabel{MV-def-emp}{{1.8}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.8}{}}
\newlabel{EM-def-emp}{{1.9}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.9}{}}
\citation{OCRF16}
\citation{Clemencon2010}
\citation{Breiman2001,Genuer2008,Biau2008,Biau2016}
\citation{Desir12,Liu2008,Shi2012}
\newlabel{eq:standard_emp_EM}{{1.10}{15}{Evaluation of anomaly detection algorithms}{equation.1.5.10}{}}
\newlabel{eq:standard_emp_MV}{{1.11}{15}{Evaluation of anomaly detection algorithms}{equation.1.5.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \nobreakspace  {}\nobreakspace  {}High-dimensional EM/MV: evaluate AD algorithms on high-dimensional data\relax }}{15}{algorithm.1}}
\newlabel{algo:EMMV}{{1}{15}{~~High-dimensional EM/MV: evaluate AD algorithms on high-dimensional data\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}One-Class Random Forests}{15}{subsection.1.5.2}}
\newlabel{resume:ocrf}{{1.5.2}{15}{One-Class Random Forests}{subsection.1.5.2}{}}
\citation{Ho1998,Panov2007}
\citation{Gini1912}
\citation{Amit1997}
\newlabel{constraint1}{{1.12}{17}{One-Class Random Forests}{equation.1.5.12}{}}
\citation{sklearn2011}
\citation{AISTAT15}
\citation{COLT15}
\citation{AISTAT16}
\citation{NIPSWORKSHOP15}
\citation{ARXIV16}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Scikit-learn contributions}{18}{section.1.6}}
\newlabel{sec:impl}{{1.6}{18}{Scikit-learn contributions}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Conclusion and Scientific Output}{18}{section.1.7}}
\newlabel{intro:concl}{{1.7}{18}{Conclusion and Scientific Output}{section.1.7}{}}
\citation{ICMLworkshop16}
\citation{NIPS16evaluation}
\citation{OCRF16}
\@writefile{toc}{\contentsline {paragraph}{Context of this work.}{19}{section.1.7}}
\@writefile{toc}{\contentsline {paragraph}{Outline of the thesis.}{19}{section.1.7}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Preliminaries}{21}{part.1}}
\citation{McDiarmid98}
\citation{Janson2002}
\citation{Massart2007,BLM2013}
\citation{Boucheron2012,Boucheron2015}
\citation{ThomasMaud2015}
\citation{Vapnik74,Devroye96,Bousquet04,BBL05,Bishop2006,Friedman2001,Vapnik2013}
\citation{Barron2003}
\newlabel{part:background}{{I}{23}{Outline of the thesis}{part.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Concentration Inequalities from the Method of bounded differences}{23}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:back_concentration}{{2}{23}{Concentration Inequalities from the Method of bounded differences}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Two fundamental results}{23}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Preliminary definitions}{23}{subsection.2.1.1}}
\citation{McDiarmid98}
\newlabel{defpreli1}{{2}{24}{}{notation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Inequality for Bounded Random Variables}{24}{subsection.2.1.2}}
\newlabel{3.14}{{2.1}{24}{}{theorem.2.1.1}{}}
\newlabel{lemme_mg}{{2.2}{24}{}{theorem.2.1.2}{}}
\citation{McDiarmid98}
\newlabel{lemme_u}{{2.3}{25}{}{theorem.2.1.3}{}}
\citation{McDiarmid98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Bernstein-type Inequality (with variance term)}{26}{subsection.2.1.3}}
\newlabel{3.15}{{2.4}{26}{}{theorem.2.1.4}{}}
\newlabel{lemme_u2}{{2.5}{26}{}{theorem.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Popular Inequalities}{27}{section.2.2}}
\newlabel{HA3.10}{{2.6}{27}{}{theorem.2.2.6}{}}
\newlabel{mcdiarmid}{{2.7}{27}{}{theorem.2.2.7}{}}
\newlabel{CL}{{2.1}{27}{}{equation.2.2.1}{}}
\newlabel{Bernstein}{{2.11}{28}{}{theorem.2.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Connections with Statistical Learning and VC theory}{29}{section.2.3}}
\newlabel{def_F}{{2.2}{29}{Connections with Statistical Learning and VC theory}{equation.2.3.2}{}}
\newlabel{eq:mcd1}{{2.5}{30}{Connections with Statistical Learning and VC theory}{equation.2.3.5}{}}
\newlabel{back:lem-rademacher}{{2.13}{30}{}{theorem.2.3.13}{}}
\newlabel{TVC}{{2.15}{31}{}{theorem.2.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Sharper VC-bounds through a Bernstein-type inequality}{31}{section.2.4}}
\newlabel{sec:concentration-contrib}{{2.4}{31}{Sharper VC-bounds through a Bernstein-type inequality}{section.2.4}{}}
\newlabel{back:lem-1}{{2.16}{32}{}{theorem.2.4.16}{}}
\newlabel{g1}{{2.6}{32}{Sharper VC-bounds through a Bernstein-type inequality}{equation.2.4.6}{}}
\newlabel{TVCsharp}{{2.18}{33}{}{theorem.2.4.18}{}}
\newlabel{eq:mcd1improved}{{2.7}{33}{}{equation.2.4.7}{}}
\citation{Massart2000}
\newlabel{back:cor_tvcsharp}{{2.20}{34}{}{theorem.2.4.20}{}}
\newlabel{back:lem-relative-rademacher}{{2.21}{34}{}{theorem.2.4.21}{}}
\citation{Leadbetter1983}
\citation{Resnick1987}
\citation{Coles2001}
\citation{BGTS04}
\citation{dHF06}
\citation{Resnick2007}
\citation{Resnick2007}
\citation{Coles2001}
\citation{Resnick2007}
\citation{Coles2001}
\citation{Segers12}
\citation{Einmahl2012}
\citation{coles1991modeling}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Extreme Value Theory}{37}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{back:EVT}{{3}{37}{Extreme Value Theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation reminder}{37}{chapter.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Univariate Extreme Value Theory}{37}{section.3.1}}
\newlabel{back:sec:1EVT}{{3.1}{37}{Univariate Extreme Value Theory}{section.3.1}{}}
\citation{DEd1989}
\citation{ELL2009}
\citation{Hill1975,Smith1987,BVT1996,Girard2004,Boucheron2015}
\citation{Beirlant2004,Chernozhukov2005,Gardes2008,Gardes2010,Girard2008,Daouia2011,Daouia2013}
\newlabel{thm:univariate-evt}{{3.1}{38}{}{theorem.3.1.1}{}}
\newlabel{eq:hyp-GEV}{{3.1}{38}{}{equation.3.1.1}{}}
\newlabel{eq:GEV}{{3.2}{38}{Univariate Extreme Value Theory}{equation.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Extreme Value Distribution with $\alpha = 2$\relax }}{39}{figure.caption.7}}
\newlabel{fig:GEV}{{3.1}{39}{Extreme Value Distribution with $\alpha = 2$\relax }{figure.caption.7}{}}
\citation{Resnick1987}
\newlabel{eq:hyp-GEV2}{{3.3}{40}{Univariate Extreme Value Theory}{equation.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Extension to the Multivariate framework}{40}{section.3.2}}
\newlabel{back:sec:MEVT}{{3.2}{40}{Extension to the Multivariate framework}{section.3.2}{}}
\newlabel{eq:hyp-GEV2-mult}{{3.4}{40}{Extension to the Multivariate framework}{equation.3.2.4}{}}
\newlabel{back:intro:regvar}{{3.5}{40}{Extension to the Multivariate framework}{equation.3.2.5}{}}
\citation{dR1977}
\citation{Resnick1987}
\newlabel{eq:regularVariation}{{3.6}{41}{Extension to the Multivariate framework}{equation.3.2.6}{}}
\newlabel{eq:pseudoPolar_change}{{3.7}{41}{Extension to the Multivariate framework}{equation.3.2.7}{}}
\newlabel{mu-phi}{{3.8}{41}{Extension to the Multivariate framework}{equation.3.2.8}{}}
\newlabel{eq:integratePhiLambda}{{3.9}{41}{Extension to the Multivariate framework}{equation.3.2.9}{}}
\newlabel{eq:limitConditAngle}{{3.10}{41}{Extension to the Multivariate framework}{equation.3.2.10}{}}
\citation{Segers12}
\newlabel{back:reg_var_U}{{3.11}{42}{Extension to the Multivariate framework}{equation.3.2.11}{}}
\newlabel{back:stdf1}{{3.12}{42}{Extension to the Multivariate framework}{equation.3.2.12}{}}
\citation{Barnett94}
\citation{Eskin2000}
\citation{Breunig2000LOF,Scholkopf2001,Steinwart2005,Scott2006,VertVert}
\citation{Shyu2003,Aggarwal2001}
\citation{Liu2008,Desir12,Shi2012}
\citation{Hodge2004survey,Chandola2009survey,Patcha2007survey,Markou2003survey}
\citation{Roberts99}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Background on classical Anomaly Detection algorithms}{43}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{back:AD_scikit}{{4}{43}{Background on classical Anomaly Detection algorithms}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}What is Anomaly Detection?}{43}{section.4.1}}
\citation{Blanchard2010,Smola2009}
\citation{Denis2005,Liu2002,Mordelet2014,duPlessis2015}
\citation{Scholkopf2001}
\citation{VertVert}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Three efficient Anomaly Detection Algorithms}{44}{section.4.2}}
\newlabel{sec:AD_sklearn}{{4.2}{44}{Three efficient Anomaly Detection Algorithms}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}One-class SVM}{44}{subsection.4.2.1}}
\newlabel{back:ocsvm}{{4.2.1}{44}{One-class SVM}{subsection.4.2.1}{}}
\citation{Thomas2015}
\citation{Bottou2007}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces SVM vs. OCSVM (hard-margin separation)\relax }}{45}{table.caption.8}}
\newlabel{table:OCSVM-hard}{{4.1}{45}{SVM vs. OCSVM (hard-margin separation)\relax }{table.caption.8}{}}
\citation{Breunig2000LOF}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces SVM vs. OCSVM ($\nu $-soft margin separation)\relax }}{46}{table.caption.9}}
\newlabel{table:OCSVM-soft}{{4.2}{46}{SVM vs. OCSVM ($\nu $-soft margin separation)\relax }{table.caption.9}{}}
\citation{Liu2008}
\citation{sklearn2011}
\citation{Liu2008}
\citation{Breunig2000LOF}
\citation{sklearn2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Local Outlier Factor algorithm}{47}{subsection.4.2.2}}
\newlabel{sec:lof}{{4.2.2}{47}{Local Outlier Factor algorithm}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Isolation Forest}{47}{subsection.4.2.3}}
\newlabel{sec:iforest}{{4.2.3}{47}{Isolation Forest}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Examples through scikit-learn}{47}{section.4.3}}
\newlabel{back:sklearn-contribution}{{4.3}{47}{Examples through scikit-learn}{section.4.3}{}}
\citation{Vanderwalt2011numpy}
\citation{Jones2015scipy}
\citation{sklearn_api2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}What is scikit-learn?}{48}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}LOF examples}{49}{subsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces LOF example\relax }}{49}{figure.caption.10}}
\newlabel{fig:lof}{{4.1}{49}{LOF example\relax }{figure.caption.10}{}}
\zref@newlabel{mdf@pagelabel-1}{\default{4.3.2}\page{49}\abspage{63}\mdf@pagevalue{49}}
\zref@newlabel{mdf@pagelabel-2}{\default{4.3.2}\page{50}\abspage{64}\mdf@pagevalue{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Isolation Forest examples}{50}{subsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Anomalies are isolated more quickly\relax }}{50}{figure.4.2}}
\newlabel{fig:ideeIF}{{4.2}{50}{Anomalies are isolated more quickly\relax }{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Convergence of the averaged depth\relax }}{50}{figure.4.3}}
\newlabel{fig:convergenceIF}{{4.3}{50}{Convergence of the averaged depth\relax }{figure.4.3}{}}
\zref@newlabel{mdf@pagelabel-3}{\default{4.3.3}\page{50}\abspage{64}\mdf@pagevalue{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Isolation Forest example\relax }}{51}{figure.caption.11}}
\newlabel{fig:iforest}{{4.4}{51}{Isolation Forest example\relax }{figure.caption.11}{}}
\zref@newlabel{mdf@pagelabel-4}{\default{4.3.3}\page{51}\abspage{65}\mdf@pagevalue{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Comparison examples}{52}{subsection.4.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces gaussian normal data with one single mode\relax }}{52}{figure.caption.12}}
\newlabel{fig:ADcomparison1}{{4.5}{52}{gaussian normal data with one single mode\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces gaussian normal data with two modes\relax }}{53}{figure.caption.13}}
\newlabel{fig:ADcomparison2}{{4.6}{53}{gaussian normal data with two modes\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces gaussian normal data with two strongly separate modes\relax }}{53}{figure.caption.14}}
\newlabel{fig:ADcomparison3}{{4.7}{53}{gaussian normal data with two strongly separate modes\relax }{figure.caption.14}{}}
\zref@newlabel{mdf@pagelabel-5}{\default{4.3.4}\page{53}\abspage{67}\mdf@pagevalue{53}}
\zref@newlabel{mdf@pagelabel-6}{\default{4.3.4}\page{54}\abspage{68}\mdf@pagevalue{54}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}An Excess-Mass based Performance Criterion}{55}{part.2}}
\citation{AISTAT15}
\citation{Kolt97,VertVert,Scholkopf2001,SHS05}
\citation{VCTWMS}
\newlabel{part:struct}{{II}{57}{Comparison examples}{part.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}On Anomaly Ranking and Excess-Mass Curves}{57}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{aistat:chap}{{5}{57}{On Anomaly Ranking and Excess-Mass Curves}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{57}{section.5.1}}
\citation{CLEM13}
\citation{Polonik95,Hartigan1987,Muller1991}
\citation{CLEM13}
\citation{CLEM13,CLEM14}
\citation{Scott2006}
\citation{CLEM13}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background and related work}{59}{section.5.2}}
\newlabel{aistat:sec:background}{{5.2}{59}{Background and related work}{section.5.2}{}}
\newlabel{aistat:eq:alpha}{{5.1}{59}{Background and related work}{equation.5.2.1}{}}
\newlabel{aistat:eq:MV}{{5.2}{59}{Background and related work}{equation.5.2.2}{}}
\citation{Polonik95}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}The Excess-Mass curve}{60}{section.5.3}}
\newlabel{aistat:sec:notations}{{5.3}{60}{The Excess-Mass curve}{section.5.3}{}}
\newlabel{aistat:solomeg}{{5.3}{60}{The Excess-Mass curve}{equation.5.3.3}{}}
\newlabel{aistat:def:opt}{{5.1}{61}{}{theorem.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces EM curves depending on densities\relax }}{61}{figure.caption.15}}
\newlabel{aistat:EMcurves}{{5.1}{61}{EM curves depending on densities\relax }{figure.caption.15}{}}
\newlabel{aistat:evident}{{5.2}{61}{}{theorem.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }}{61}{figure.caption.16}}
\newlabel{aistat:MVcurve}{{5.2}{61}{Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }{figure.caption.16}{}}
\newlabel{aistat:derive}{{5.3}{62}{}{theorem.5.3.3}{}}
\newlabel{aistat:EM}{{5.4}{62}{}{equation.5.3.4}{}}
\newlabel{aistat:score_cont}{{5.5}{62}{The Excess-Mass curve}{equation.5.3.5}{}}
\newlabel{aistat:propestim}{{5.5}{62}{}{theorem.5.3.5}{}}
\citation{CLEM13}
\citation{Kolt06}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}A general approach to learn a scoring function}{63}{section.5.4}}
\newlabel{aistat:sec:estim}{{5.4}{63}{A general approach to learn a scoring function}{section.5.4}{}}
\citation{Kolt06}
\citation{Polonik1998}
\newlabel{aistat:penality}{{5.6}{64}{A general approach to learn a scoring function}{equation.5.4.6}{}}
\newlabel{aistat:propmono}{{5.6}{64}{}{theorem.5.4.6}{}}
\newlabel{aistat:score}{{5.7}{64}{A general approach to learn a scoring function}{equation.5.4.7}{}}
\newlabel{aistat:orderscore}{{5.7}{64}{}{theorem.5.4.7}{}}
\newlabel{aistat:mono}{{5.9}{65}{}{theorem.5.4.9}{}}
\newlabel{aistat:compact_support_case}{{5.10}{65}{}{theorem.5.4.10}{}}
\newlabel{aistat:supf}{{5.11}{65}{}{theorem.5.4.11}{}}
\newlabel{aistat:theofini}{{5.12}{65}{}{theorem.5.4.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Extensions - Further results}{65}{section.5.5}}
\newlabel{aistat:sec:ext}{{5.5}{65}{Extensions - Further results}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Distributions with non compact support}{66}{subsection.5.5.1}}
\newlabel{aistat:sec:infiniteSupport}{{5.5.1}{66}{Distributions with non compact support}{subsection.5.5.1}{}}
\newlabel{aistat:extension_non_compact}{{5.5.1}{66}{Distributions with non compact support}{subsection.5.5.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \nobreakspace  {}\nobreakspace  {}Learning a scoring function\relax }}{66}{algorithm.2}}
\newlabel{aistat:algo1}{{2}{66}{~~Learning a scoring function\relax }{algorithm.2}{}}
\newlabel{aistat:definition_sN}{{5.8}{66}{~~Learning a scoring function\relax }{equation.5.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Unsuccessful mass-volume criterion optimization\relax }}{66}{figure.caption.17}}
\newlabel{aistat:algo-problem}{{5.3}{66}{Unsuccessful mass-volume criterion optimization\relax }{figure.caption.17}{}}
\newlabel{aistat:tk-omegak}{{5.9}{67}{Distributions with non compact support}{equation.5.5.9}{}}
\newlabel{aistat:tk1}{{5.10}{67}{Distributions with non compact support}{equation.5.5.10}{}}
\newlabel{aistat:fondineq}{{5.11}{67}{Distributions with non compact support}{equation.5.5.11}{}}
\newlabel{aistat:tk}{{5.12}{67}{Distributions with non compact support}{equation.5.5.12}{}}
\newlabel{aistat:thmprinc}{{5.13}{67}{}{theorem.5.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Bias analysis}{68}{subsection.5.5.2}}
\newlabel{aistat:biais}{{5.5.2}{68}{Bias analysis}{subsection.5.5.2}{}}
\newlabel{aistat:thmprincbiais}{{5.14}{68}{}{theorem.5.5.14}{}}
\newlabel{aistat:propbiais}{{5.16}{68}{}{theorem.5.5.16}{}}
\citation{CLEM13}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Simulation examples}{69}{section.5.6}}
\newlabel{aistat:sec:simul}{{5.6}{69}{Simulation examples}{section.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusion}{69}{section.5.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Optimal and realized EM curves\relax }}{70}{figure.caption.18}}
\newlabel{aistat:EMMS}{{5.4}{70}{Optimal and realized EM curves\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Zoom near 0 \nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\relax }}{70}{figure.caption.18}}
\newlabel{aistat:EMMSzoom}{{5.5}{70}{Zoom near 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces $EM_\mathcal  {G}$ for different $l$\relax }}{70}{figure.caption.19}}
\newlabel{aistat:EMGEM}{{5.6}{70}{$EM_\mathcal {G}$ for different $l$\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Illustrations}{71}{section.5.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces density and scoring functions\relax }}{71}{figure.caption.20}}
\newlabel{aistat:scoring3D}{{5.7}{71}{density and scoring functions\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Detailed Proofs}{71}{section.5.9}}
\newlabel{aistat:sec:detailed_proofs}{{5.9}{71}{Detailed Proofs}{section.5.9}{}}
\citation{Federer1969}
\newlabel{aistat:lemmeMs}{{5.17}{74}{}{theorem.5.9.17}{}}
\newlabel{aistat:lemmederive}{{5.18}{75}{}{theorem.5.9.18}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Accuracy on Extreme Regions}{77}{part.3}}
\citation{COLT15}
\citation{Huangphd}
\citation{Drees98}
\citation{Embrechts2000}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\newlabel{part:vect}{{III}{79}{}{part.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning the dependence structure of rare events: a non-asymptotic study}{79}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{colt}{{6}{79}{Learning the dependence structure of rare events: a non-asymptotic study}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{79}{section.6.1}}
\newlabel{colt:sec:intro}{{6.1}{79}{Introduction}{section.6.1}{}}
\citation{Resnick2007}
\citation{Resnick1987}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Background on the stable tail dependence function}{80}{section.6.2}}
\newlabel{colt:sec:background}{{6.2}{80}{Background on the stable tail dependence function}{section.6.2}{}}
\newlabel{colt:stdf1}{{6.1}{80}{Background on the stable tail dependence function}{equation.6.2.1}{}}
\citation{Falk94}
\citation{Falk94}
\citation{Vapnik74}
\citation{Anthony93}
\citation{Bousquet04}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}A VC-type inequality adapted to the study of low probability regions}{81}{section.6.3}}
\newlabel{colt:sec:concentration}{{6.3}{81}{A VC-type inequality adapted to the study of low probability regions}{section.6.3}{}}
\newlabel{colt:thm-princ}{{6.1}{81}{}{theorem.6.3.1}{}}
\newlabel{colt:thm-princ-ineq}{{6.2}{81}{}{equation.6.3.2}{}}
\citation{Bousquet04}
\newlabel{colt:normalize-vc}{{6.3}{82}{}{equation.6.3.3}{}}
\newlabel{colt:rk:interpretation}{{6.4}{82}{}{theorem.6.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Classification of Extremes}{82}{theorem.6.3.4}}
\newlabel{colt:prediction:rates}{{6.4}{83}{Classification of Extremes}{equation.6.3.4}{}}
\newlabel{rk:classif-details}{{6.5}{83}{}{theorem.6.3.5}{}}
\citation{Huangphd}
\citation{Qi97}
\citation{Drees98}
\citation{Einmahl2006}
\citation{Huangphd}
\citation{Drees98}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\citation{Devroye96}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}A bound on the STDF}{84}{section.6.4}}
\newlabel{colt:sec:stdf}{{6.4}{84}{A bound on the STDF}{section.6.4}{}}
\newlabel{colt:stdf}{{6.5}{84}{A bound on the STDF}{equation.6.4.5}{}}
\newlabel{colt:ln}{{6.6}{84}{A bound on the STDF}{equation.6.4.6}{}}
\citation{ELL2009}
\citation{Qi97}
\newlabel{colt:Qialt2}{{6.7}{85}{A bound on the STDF}{equation.6.4.7}{}}
\newlabel{colt:thm:l}{{6.6}{85}{}{theorem.6.4.6}{}}
\newlabel{colt:thm:l:ineq}{{6.8}{85}{}{equation.6.4.8}{}}
\newlabel{colt:ln-Fn}{{6.7}{85}{Link between $l_n$ and $\tilde F_n$}{theorem.6.4.7}{}}
\citation{Wellner78}
\newlabel{colt:Fn-tildeF}{{6.8}{86}{Uniform bound on $\tilde F_n$'s deviations}{theorem.6.4.8}{}}
\newlabel{colt:U-x}{{6.9}{86}{Bound on the order statistics of $\mb U$}{theorem.6.4.9}{}}
\newlabel{colt:eq-Wellner}{{6.9}{86}{Bound on the order statistics of $\mb U$}{equation.6.4.9}{}}
\citation{Qi97}
\newlabel{colt:unif_conv}{{6.10}{87}{A bound on the STDF}{equation.6.4.10}{}}
\citation{deHaan1996}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Discussion}{88}{section.6.5}}
\newlabel{colt:sec:conclusion}{{6.5}{88}{Discussion}{section.6.5}{}}
\citation{ARXIV16}
\citation{AISTAT15}
\citation{NIPSWORKSHOP15}
\citation{finkenstadt2003extreme,smith2003statistics}
\citation{Clifton2011,Lee2008}
\citation{Resnick1987}
\citation{BGTS04}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Sparse Representation of Multivariate Extremes}{91}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{jmva}{{7}{91}{Sparse Representation of Multivariate Extremes}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{91}{section.7.1}}
\newlabel{jmva:sec:intro}{{7.1}{91}{Introduction}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Context: multivariate extreme values in large dimension}{91}{subsection.7.1.1}}
\citation{coles1991modeling,fougeres2009models,cooley2010pairwise,sabourinNaveau2012}
\citation{Segers12Bernoulli,Drees98,Embrechts2000,Einmahl2012,dHF06}
\citation{Einmahl2001}
\citation{Einmahl2009}
\citation{stephenson2009high}
\citation{sabourinNaveau2012}
\citation{COLT15}
\citation{Einmahl2001}
\citation{Einmahl2009}
\citation{Roberts99}
\citation{Roberts2000}
\citation{Clifton2011}
\citation{Clifton2008}
\citation{Lee2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Application to Anomaly Detection}{93}{subsection.7.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Multivariate EVT Framework and Problem Statement}{94}{section.7.2}}
\newlabel{jmva:sec:framework}{{7.2}{94}{Multivariate EVT Framework and Problem Statement}{section.7.2}{}}
\newlabel{jmva:intro:assumption2}{{7.1}{94}{Multivariate EVT Framework and Problem Statement}{equation.7.2.1}{}}
\newlabel{jmva:intro:regvar}{{7.2}{94}{Multivariate EVT Framework and Problem Statement}{equation.7.2.2}{}}
\newlabel{jmva:eq:regularVariation}{{7.3}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.3}{}}
\newlabel{jmva:eq:pseudoPolar_change}{{7.4}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.4}{}}
\newlabel{jmva:mu-phi}{{7.5}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.5}{}}
\newlabel{jmva:eq:integratePhiLambda}{{7.6}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.6}{}}
\newlabel{jmva:eq:limitConditAngle}{{7.7}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Statement of the Statistical Problem}{95}{subsection.7.2.1}}
\newlabel{jmva:sec:decomposMu}{{7.2.1}{95}{Statement of the Statistical Problem}{subsection.7.2.1}{}}
\newlabel{jmva:cone}{{7.8}{95}{Statement of the Statistical Problem}{equation.7.2.8}{}}
\newlabel{jmva:eq:decomp1}{{7.9}{96}{Statement of the Statistical Problem}{equation.7.2.9}{}}
\newlabel{jmva:eq:epsilon_Rectangle}{{7.10}{96}{Statement of the Statistical Problem}{equation.7.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Truncated cones in 3D\relax }}{96}{figure.7.1}}
\newlabel{jmva:fig:3Dcones}{{7.1}{96}{Truncated cones in 3D\relax }{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Truncated $\epsilon $-rectangles in 2D\relax }}{96}{figure.7.2}}
\newlabel{jmva:2Dcones}{{7.2}{96}{Truncated $\epsilon $-rectangles in 2D\relax }{figure.7.2}{}}
\newlabel{jmva:lem:limit_muCalphaEps}{{7.1}{96}{}{theorem.7.2.1}{}}
\newlabel{jmva:rk_approx_mu_n}{{7.2}{97}{}{theorem.7.2.2}{}}
\citation{Einmahl2009}
\newlabel{jmva:eq:representation_M}{{7.13}{98}{Statement of the Statistical Problem}{equation.7.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Regularity Assumptions}{98}{subsection.7.2.2}}
\newlabel{jmva:sec:RegularAssumptions}{{7.2.2}{98}{Regularity Assumptions}{subsection.7.2.2}{}}
\newlabel{jmva:hypo:continuous_margins}{{1}{98}{}{assumption.1}{}}
\newlabel{jmva:hypo:continuousMu}{{2}{98}{}{assumption.2}{}}
\newlabel{jmva:lem:continuousPhi}{{7.3}{98}{}{theorem.7.2.3}{}}
\newlabel{jmva:eq:decomposePhi}{{7.15}{99}{Regularity Assumptions}{equation.7.2.15}{}}
\newlabel{jmva:eq:supDensity}{{7.16}{99}{Regularity Assumptions}{equation.7.2.16}{}}
\newlabel{jmva:hypo:abs_continuousPhi}{{3}{99}{}{assumption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}A non-parametric estimator of the subcones' mass : definition and preliminary results}{99}{section.7.3}}
\newlabel{jmva:sec:estimation}{{7.3}{99}{A non-parametric estimator of the subcones' mass : definition and preliminary results}{section.7.3}{}}
\citation{Einmahl2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}A natural empirical version of the exponent measure mu}{100}{subsection.7.3.1}}
\newlabel{jmva:sec:classicEstimators}{{7.3.1}{100}{A natural empirical version of the exponent measure mu}{subsection.7.3.1}{}}
\newlabel{jmva:def:transform}{{7.17}{100}{A natural empirical version of the exponent measure mu}{equation.7.3.17}{}}
\newlabel{jmva:mu_n}{{7.18}{100}{A natural empirical version of the exponent measure mu}{equation.7.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Accounting for the non asymptotic nature of data: epsilon-thickening.}{100}{subsection.7.3.2}}
\newlabel{jmva:heuristic_mu_n}{{7.19}{100}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Estimation procedure\relax }}{101}{figure.caption.21}}
\newlabel{jmva:estimation_rect}{{7.3}{101}{Estimation procedure\relax }{figure.caption.21}{}}
\newlabel{jmva:eq:interprete_mun_Pcondit}{{7.20}{101}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.20}{}}
\newlabel{jmva:error_decomp}{{7.21}{101}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Preliminaries: uniform approximation over a VC-class of rectangles}{101}{subsection.7.3.3}}
\newlabel{jmva:sec:rectangles}{{7.3.3}{101}{Preliminaries: uniform approximation over a VC-class of rectangles}{subsection.7.3.3}{}}
\citation{Huangphd}
\citation{Qi97}
\citation{Drees98}
\citation{Einmahl2006}
\citation{COLT15}
\citation{Huangphd}
\citation{Drees98}
\citation{Embrechts2000}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\citation{COLT15}
\newlabel{jmva:stdf}{{7.22}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.22}{}}
\newlabel{jmva:def:tildeF}{{7.23}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.23}{}}
\newlabel{jmva:empir-Stdf}{{7.24}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.24}{}}
\newlabel{jmva:lem:equivalence}{{7.5}{102}{}{theorem.7.3.5}{}}
\newlabel{jmva:empir-Stdf2}{{7.25}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.25}{}}
\newlabel{jmva:set-R}{{7.26}{103}{Preliminaries: uniform approximation over a VC-class of rec