\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Contributions}{iii}{dummy.1}}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{dummy.2}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xiii}{dummy.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Summary}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{1}{Summary}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}}
\citation{Scholkopf2001,Steinwart2005,Scott2006,VertVert}
\citation{Scholkopf2001,Scott2006,VertVert}
\citation{Tsybakov1997,Cuevas1997,Baillo2001,Baillo2003,Cadre2006,Rigollet2009,Mason2009}
\@writefile{toc}{\contentsline {paragraph}{Notations}{2}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Anomaly Detection, Anomaly Ranking and Scoring Functions}{2}{section.1.2}}
\newlabel{resume:scoring_function}{{1.2}{2}{Anomaly Detection, Anomaly Ranking and Scoring Functions}{section.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Summary of notation}}{3}{table.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notations}{{1.1}{3}{Summary of notation}{table.caption.5}{}}
\citation{Liu2008}
\citation{AISTAT15}
\citation{CLEM13}
\citation{Polonik97,Einmahl1992}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}M-estimation and criteria for scoring functions}{4}{section.1.3}}
\newlabel{resume:scoring}{{1.3}{4}{M-estimation and criteria for scoring functions}{section.1.3}{}}
\citation{Polonik97}
\citation{Scott2006}
\citation{CLEM13,CLEM14}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Minimum Volume sets}{5}{subsection.1.3.1}}
\newlabel{resume:mv-set}{{1.3.1}{5}{Minimum Volume sets}{subsection.1.3.1}{}}
\newlabel{eq:MV}{{1.1}{5}{Minimum Volume sets}{equation.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Mass-Volume curve}{5}{subsection.1.3.2}}
\newlabel{resume:mv-curve}{{1.3.2}{5}{Mass-Volume curve}{subsection.1.3.2}{}}
\newlabel{eq:alpha_beta}{{1.2}{6}{Mass-Volume curve}{equation.1.3.2}{}}
\newlabel{def:MV}{{1.3}{6}{Mass-Volume curve}{equation.1.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Mass-Volume at level $\alpha $\relax }}{6}{figure.caption.6}}
\newlabel{MVcurve}{{1.1}{6}{Mass-Volume at level $\alpha $\relax }{figure.caption.6}{}}
\citation{Polonik95}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}The Excess-Mass criterion}{7}{subsection.1.3.3}}
\newlabel{resume:em-curve}{{1.3.3}{7}{The Excess-Mass criterion}{subsection.1.3.3}{}}
\newlabel{solomeg}{{1.4}{7}{The Excess-Mass criterion}{equation.1.3.4}{}}
\newlabel{EM}{{1.5}{7}{The Excess-Mass criterion}{equation.1.3.5}{}}
\newlabel{EMcurve}{{1.3.3}{7}{The Excess-Mass criterion}{equation.1.3.5}{}}
\citation{AISTAT15}
\citation{CLEM13}
\citation{COLT15}
\citation{Huangphd,Drees98,Embrechts2000,dHF06}
\citation{Qi97,Einmahl2012}
\citation{COLT15}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Accuracy on extreme regions}{9}{section.1.4}}
\newlabel{resume:extreme}{{1.4}{9}{Accuracy on extreme regions}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Extreme Values Analysis through STDF estimation}{9}{subsection.1.4.1}}
\newlabel{resume:stdf}{{1.4.1}{9}{Extreme Values Analysis through STDF estimation}{subsection.1.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries.}{9}{subsection.1.4.1}}
\citation{Einmahl2001,Einmahl2009}
\@writefile{toc}{\contentsline {paragraph}{Learning the dependence structure of rare events.}{10}{subsection.1.4.1}}
\newlabel{thm-princ}{{1.2}{10}{}{theorem.1.4.2}{}}
\newlabel{thm:l}{{1.3}{10}{}{theorem.1.4.3}{}}
\citation{AISTAT16}
\citation{ARXIV16}
\citation{Roberts99,Roberts2000,Clifton2011,Clifton2008,Lee2008}
\citation{coles1991modeling,fougeres2009models,cooley2010pairwise,sabourinNaveau2012}
\citation{stephenson2009high}
\citation{Einmahl2001,Einmahl2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Sparse Representation of Multivariate Extremes}{11}{subsection.1.4.2}}
\newlabel{resume:sec:JMVA}{{1.4.2}{11}{Sparse Representation of Multivariate Extremes}{subsection.1.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Truncated cones in 3D\relax }}{12}{figure.1.2}}
\newlabel{fig:3Dcones}{{1.2}{12}{Truncated cones in 3D\relax }{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Truncated $\epsilon $-cones in 2D\relax }}{12}{figure.1.3}}
\newlabel{2Dcones}{{1.3}{12}{Truncated $\epsilon $-cones in 2D\relax }{figure.1.3}{}}
\citation{ICMLworkshop16}
\citation{NIPS16evaluation}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Heuristic approaches}{13}{section.1.5}}
\newlabel{resume:sec:heuristic}{{1.5}{13}{Heuristic approaches}{section.1.5}{}}
\citation{Provost1997,Provost1998,Fawcett2006}
\citation{Davis2006,Clemencon2009}
\citation{Thomas2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Evaluation of anomaly detection algorithms}{14}{subsection.1.5.1}}
\newlabel{resume:evaluation}{{1.5.1}{14}{Evaluation of anomaly detection algorithms}{subsection.1.5.1}{}}
\newlabel{intro:MV-def}{{1.6}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.6}{}}
\newlabel{intro:EM-def}{{1.7}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.7}{}}
\newlabel{MV-def-emp}{{1.8}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.8}{}}
\newlabel{EM-def-emp}{{1.9}{14}{Evaluation of anomaly detection algorithms}{equation.1.5.9}{}}
\citation{OCRF16}
\citation{Clemencon2010}
\citation{Breiman2001,Genuer2008,Biau2008,Biau2016}
\citation{Desir12,Liu2008,Shi2012}
\newlabel{eq:standard_emp_EM}{{1.10}{15}{Evaluation of anomaly detection algorithms}{equation.1.5.10}{}}
\newlabel{eq:standard_emp_MV}{{1.11}{15}{Evaluation of anomaly detection algorithms}{equation.1.5.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \nobreakspace  {}\nobreakspace  {}High-dimensional EM/MV: evaluate AD algorithms on high-dimensional data\relax }}{15}{algorithm.1}}
\newlabel{algo:EMMV}{{1}{15}{~~High-dimensional EM/MV: evaluate AD algorithms on high-dimensional data\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}One-Class Random Forests}{15}{subsection.1.5.2}}
\newlabel{resume:ocrf}{{1.5.2}{15}{One-Class Random Forests}{subsection.1.5.2}{}}
\citation{Ho1998,Panov2007}
\citation{Gini1912}
\citation{Amit1997}
\newlabel{constraint1}{{1.12}{17}{One-Class Random Forests}{equation.1.5.12}{}}
\citation{sklearn2011}
\citation{AISTAT15}
\citation{COLT15}
\citation{AISTAT16}
\citation{NIPSWORKSHOP15}
\citation{ARXIV16}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Scikit-learn contributions}{18}{section.1.6}}
\newlabel{sec:impl}{{1.6}{18}{Scikit-learn contributions}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Conclusion and Scientific Output}{18}{section.1.7}}
\newlabel{intro:concl}{{1.7}{18}{Conclusion and Scientific Output}{section.1.7}{}}
\citation{ICMLworkshop16}
\citation{NIPS16evaluation}
\citation{OCRF16}
\@writefile{toc}{\contentsline {paragraph}{Context of this work.}{19}{section.1.7}}
\@writefile{toc}{\contentsline {paragraph}{Outline of the thesis.}{19}{section.1.7}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Preliminaries}{21}{part.1}}
\citation{McDiarmid98}
\citation{Janson2002}
\citation{Massart2007,BLM2013}
\citation{Boucheron2012,Boucheron2015}
\citation{ThomasMaud2015}
\citation{Vapnik74,Devroye96,Bousquet04,BBL05,Bishop2006,Friedman2001,Vapnik2013}
\citation{Barron2003}
\newlabel{part:background}{{I}{23}{Outline of the thesis}{part.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Concentration Inequalities from the Method of bounded differences}{23}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:back_concentration}{{2}{23}{Concentration Inequalities from the Method of bounded differences}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Two fundamental results}{23}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Preliminary definitions}{23}{subsection.2.1.1}}
\citation{McDiarmid98}
\newlabel{defpreli1}{{2}{24}{}{notation.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Inequality for Bounded Random Variables}{24}{subsection.2.1.2}}
\newlabel{3.14}{{2.1}{24}{}{theorem.2.1.1}{}}
\newlabel{lemme_mg}{{2.2}{24}{}{theorem.2.1.2}{}}
\citation{McDiarmid98}
\newlabel{lemme_u}{{2.3}{25}{}{theorem.2.1.3}{}}
\citation{McDiarmid98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Bernstein-type Inequality (with variance term)}{26}{subsection.2.1.3}}
\newlabel{3.15}{{2.4}{26}{}{theorem.2.1.4}{}}
\newlabel{lemme_u2}{{2.5}{26}{}{theorem.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Popular Inequalities}{27}{section.2.2}}
\newlabel{HA3.10}{{2.6}{27}{}{theorem.2.2.6}{}}
\newlabel{mcdiarmid}{{2.7}{27}{}{theorem.2.2.7}{}}
\newlabel{CL}{{2.1}{27}{}{equation.2.2.1}{}}
\newlabel{Bernstein}{{2.11}{28}{}{theorem.2.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Connections with Statistical Learning and VC theory}{29}{section.2.3}}
\newlabel{def_F}{{2.2}{29}{Connections with Statistical Learning and VC theory}{equation.2.3.2}{}}
\newlabel{eq:mcd1}{{2.5}{30}{Connections with Statistical Learning and VC theory}{equation.2.3.5}{}}
\newlabel{back:lem-rademacher}{{2.13}{30}{}{theorem.2.3.13}{}}
\newlabel{TVC}{{2.15}{31}{}{theorem.2.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Sharper VC-bounds through a Bernstein-type inequality}{31}{section.2.4}}
\newlabel{sec:concentration-contrib}{{2.4}{31}{Sharper VC-bounds through a Bernstein-type inequality}{section.2.4}{}}
\newlabel{back:lem-1}{{2.16}{32}{}{theorem.2.4.16}{}}
\newlabel{g1}{{2.6}{32}{Sharper VC-bounds through a Bernstein-type inequality}{equation.2.4.6}{}}
\newlabel{TVCsharp}{{2.18}{33}{}{theorem.2.4.18}{}}
\newlabel{eq:mcd1improved}{{2.7}{33}{}{equation.2.4.7}{}}
\citation{Massart2000}
\newlabel{back:cor_tvcsharp}{{2.20}{34}{}{theorem.2.4.20}{}}
\newlabel{back:lem-relative-rademacher}{{2.21}{34}{}{theorem.2.4.21}{}}
\citation{Leadbetter1983}
\citation{Resnick1987}
\citation{Coles2001}
\citation{BGTS04}
\citation{dHF06}
\citation{Resnick2007}
\citation{Resnick2007}
\citation{Coles2001}
\citation{Resnick2007}
\citation{Coles2001}
\citation{Segers12}
\citation{Einmahl2012}
\citation{coles1991modeling}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Extreme Value Theory}{37}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{back:EVT}{{3}{37}{Extreme Value Theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation reminder}{37}{chapter.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Univariate Extreme Value Theory}{37}{section.3.1}}
\newlabel{back:sec:1EVT}{{3.1}{37}{Univariate Extreme Value Theory}{section.3.1}{}}
\citation{DEd1989}
\citation{ELL2009}
\citation{Hill1975,Smith1987,BVT1996,Girard2004,Boucheron2015}
\citation{Beirlant2004,Chernozhukov2005,Gardes2008,Gardes2010,Girard2008,Daouia2011,Daouia2013}
\newlabel{thm:univariate-evt}{{3.1}{38}{}{theorem.3.1.1}{}}
\newlabel{eq:hyp-GEV}{{3.1}{38}{}{equation.3.1.1}{}}
\newlabel{eq:GEV}{{3.2}{38}{Univariate Extreme Value Theory}{equation.3.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Extreme Value Distribution with $\alpha = 2$\relax }}{39}{figure.caption.7}}
\newlabel{fig:GEV}{{3.1}{39}{Extreme Value Distribution with $\alpha = 2$\relax }{figure.caption.7}{}}
\citation{Resnick1987}
\newlabel{eq:hyp-GEV2}{{3.3}{40}{Univariate Extreme Value Theory}{equation.3.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Extension to the Multivariate framework}{40}{section.3.2}}
\newlabel{back:sec:MEVT}{{3.2}{40}{Extension to the Multivariate framework}{section.3.2}{}}
\newlabel{eq:hyp-GEV2-mult}{{3.4}{40}{Extension to the Multivariate framework}{equation.3.2.4}{}}
\newlabel{back:intro:regvar}{{3.5}{40}{Extension to the Multivariate framework}{equation.3.2.5}{}}
\citation{dR1977}
\citation{Resnick1987}
\newlabel{eq:regularVariation}{{3.6}{41}{Extension to the Multivariate framework}{equation.3.2.6}{}}
\newlabel{eq:pseudoPolar_change}{{3.7}{41}{Extension to the Multivariate framework}{equation.3.2.7}{}}
\newlabel{mu-phi}{{3.8}{41}{Extension to the Multivariate framework}{equation.3.2.8}{}}
\newlabel{eq:integratePhiLambda}{{3.9}{41}{Extension to the Multivariate framework}{equation.3.2.9}{}}
\newlabel{eq:limitConditAngle}{{3.10}{41}{Extension to the Multivariate framework}{equation.3.2.10}{}}
\citation{Segers12}
\newlabel{back:reg_var_U}{{3.11}{42}{Extension to the Multivariate framework}{equation.3.2.11}{}}
\newlabel{back:stdf1}{{3.12}{42}{Extension to the Multivariate framework}{equation.3.2.12}{}}
\citation{Barnett94}
\citation{Eskin2000}
\citation{Breunig2000LOF,Scholkopf2001,Steinwart2005,Scott2006,VertVert}
\citation{Shyu2003,Aggarwal2001}
\citation{Liu2008,Desir12,Shi2012}
\citation{Hodge2004survey,Chandola2009survey,Patcha2007survey,Markou2003survey}
\citation{Roberts99}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Background on classical Anomaly Detection algorithms}{43}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{back:AD_scikit}{{4}{43}{Background on classical Anomaly Detection algorithms}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}What is Anomaly Detection?}{43}{section.4.1}}
\citation{Blanchard2010,Smola2009}
\citation{Denis2005,Liu2002,Mordelet2014,duPlessis2015}
\citation{Scholkopf2001}
\citation{VertVert}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Three efficient Anomaly Detection Algorithms}{44}{section.4.2}}
\newlabel{sec:AD_sklearn}{{4.2}{44}{Three efficient Anomaly Detection Algorithms}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}One-class SVM}{44}{subsection.4.2.1}}
\newlabel{back:ocsvm}{{4.2.1}{44}{One-class SVM}{subsection.4.2.1}{}}
\citation{Thomas2015}
\citation{Bottou2007}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces SVM vs. OCSVM (hard-margin separation)\relax }}{45}{table.caption.8}}
\newlabel{table:OCSVM-hard}{{4.1}{45}{SVM vs. OCSVM (hard-margin separation)\relax }{table.caption.8}{}}
\citation{Breunig2000LOF}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces SVM vs. OCSVM ($\nu $-soft margin separation)\relax }}{46}{table.caption.9}}
\newlabel{table:OCSVM-soft}{{4.2}{46}{SVM vs. OCSVM ($\nu $-soft margin separation)\relax }{table.caption.9}{}}
\citation{Liu2008}
\citation{sklearn2011}
\citation{Liu2008}
\citation{Breunig2000LOF}
\citation{sklearn2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Local Outlier Factor algorithm}{47}{subsection.4.2.2}}
\newlabel{sec:lof}{{4.2.2}{47}{Local Outlier Factor algorithm}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Isolation Forest}{47}{subsection.4.2.3}}
\newlabel{sec:iforest}{{4.2.3}{47}{Isolation Forest}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Examples through scikit-learn}{47}{section.4.3}}
\newlabel{back:sklearn-contribution}{{4.3}{47}{Examples through scikit-learn}{section.4.3}{}}
\citation{Vanderwalt2011numpy}
\citation{Jones2015scipy}
\citation{sklearn_api2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}What is scikit-learn?}{48}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}LOF examples}{49}{subsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces LOF example\relax }}{49}{figure.caption.10}}
\newlabel{fig:lof}{{4.1}{49}{LOF example\relax }{figure.caption.10}{}}
\zref@newlabel{mdf@pagelabel-1}{\default{4.3.2}\page{49}\abspage{63}\mdf@pagevalue{49}}
\zref@newlabel{mdf@pagelabel-2}{\default{4.3.2}\page{50}\abspage{64}\mdf@pagevalue{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Isolation Forest examples}{50}{subsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Anomalies are isolated more quickly\relax }}{50}{figure.4.2}}
\newlabel{fig:ideeIF}{{4.2}{50}{Anomalies are isolated more quickly\relax }{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Convergence of the averaged depth\relax }}{50}{figure.4.3}}
\newlabel{fig:convergenceIF}{{4.3}{50}{Convergence of the averaged depth\relax }{figure.4.3}{}}
\zref@newlabel{mdf@pagelabel-3}{\default{4.3.3}\page{50}\abspage{64}\mdf@pagevalue{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Isolation Forest example\relax }}{51}{figure.caption.11}}
\newlabel{fig:iforest}{{4.4}{51}{Isolation Forest example\relax }{figure.caption.11}{}}
\zref@newlabel{mdf@pagelabel-4}{\default{4.3.3}\page{51}\abspage{65}\mdf@pagevalue{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Comparison examples}{52}{subsection.4.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Gaussian normal data with one single mode\relax }}{52}{figure.caption.12}}
\newlabel{fig:ADcomparison1}{{4.5}{52}{Gaussian normal data with one single mode\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Gaussian normal data with two modes\relax }}{53}{figure.caption.13}}
\newlabel{fig:ADcomparison2}{{4.6}{53}{Gaussian normal data with two modes\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Gaussian normal data with two strongly separate modes\relax }}{53}{figure.caption.14}}
\newlabel{fig:ADcomparison3}{{4.7}{53}{Gaussian normal data with two strongly separate modes\relax }{figure.caption.14}{}}
\zref@newlabel{mdf@pagelabel-5}{\default{4.3.4}\page{53}\abspage{67}\mdf@pagevalue{53}}
\zref@newlabel{mdf@pagelabel-6}{\default{4.3.4}\page{54}\abspage{68}\mdf@pagevalue{54}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}An Excess-Mass based Performance Criterion}{55}{part.2}}
\citation{AISTAT15}
\citation{Kolt97,VertVert,Scholkopf2001,Steinwart2005}
\citation{VCTWMS}
\newlabel{part:struct}{{II}{57}{Comparison examples}{part.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}On Anomaly Ranking and Excess-Mass Curves}{57}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{aistat:chap}{{5}{57}{On Anomaly Ranking and Excess-Mass Curves}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{57}{section.5.1}}
\citation{CLEM13}
\citation{Polonik95,Hartigan1987,Muller1991}
\citation{CLEM13}
\citation{CLEM13,CLEM14}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background and related work}{59}{section.5.2}}
\newlabel{aistat:sec:background}{{5.2}{59}{Background and related work}{section.5.2}{}}
\newlabel{aistat:eq:alpha}{{5.1}{59}{Background and related work}{equation.5.2.1}{}}
\newlabel{aistat:eq:MV}{{5.2}{59}{Background and related work}{equation.5.2.2}{}}
\citation{Scott2006}
\citation{CLEM13}
\citation{Polonik95}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}The Excess-Mass curve}{60}{section.5.3}}
\newlabel{aistat:sec:notations}{{5.3}{60}{The Excess-Mass curve}{section.5.3}{}}
\newlabel{aistat:solomeg}{{5.3}{60}{The Excess-Mass curve}{equation.5.3.3}{}}
\newlabel{aistat:def:opt}{{5.1}{61}{}{theorem.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces EM curves depending on densities\relax }}{61}{figure.caption.15}}
\newlabel{aistat:EMcurves}{{5.1}{61}{EM curves depending on densities\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }}{61}{figure.caption.16}}
\newlabel{aistat:MVcurve}{{5.2}{61}{Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }{figure.caption.16}{}}
\newlabel{aistat:evident}{{5.2}{62}{}{theorem.5.3.2}{}}
\newlabel{aistat:derive}{{5.3}{62}{}{theorem.5.3.3}{}}
\newlabel{aistat:EM}{{5.4}{62}{}{equation.5.3.4}{}}
\newlabel{aistat:score_cont}{{5.5}{62}{The Excess-Mass curve}{equation.5.3.5}{}}
\newlabel{aistat:propestim}{{5.5}{62}{}{theorem.5.3.5}{}}
\citation{CLEM13}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}A general approach to learn a scoring function}{63}{section.5.4}}
\newlabel{aistat:sec:estim}{{5.4}{63}{A general approach to learn a scoring function}{section.5.4}{}}
\citation{Kolt06}
\citation{Kolt06}
\newlabel{aistat:penality}{{5.6}{64}{A general approach to learn a scoring function}{equation.5.4.6}{}}
\newlabel{aistat:propmono}{{5.6}{64}{}{theorem.5.4.6}{}}
\newlabel{aistat:score}{{5.7}{64}{A general approach to learn a scoring function}{equation.5.4.7}{}}
\citation{Polonik1998}
\newlabel{aistat:orderscore}{{5.7}{65}{}{theorem.5.4.7}{}}
\newlabel{aistat:mono}{{5.9}{65}{}{theorem.5.4.9}{}}
\newlabel{aistat:compact_support_case}{{5.10}{65}{}{theorem.5.4.10}{}}
\newlabel{aistat:supf}{{5.11}{65}{}{theorem.5.4.11}{}}
\newlabel{aistat:theofini}{{5.12}{65}{}{theorem.5.4.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Extensions - Further results}{66}{section.5.5}}
\newlabel{aistat:sec:ext}{{5.5}{66}{Extensions - Further results}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Distributions with non compact support}{66}{subsection.5.5.1}}
\newlabel{aistat:sec:infiniteSupport}{{5.5.1}{66}{Distributions with non compact support}{subsection.5.5.1}{}}
\newlabel{aistat:extension_non_compact}{{5.5.1}{66}{Distributions with non compact support}{subsection.5.5.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \nobreakspace  {}\nobreakspace  {}Learning a scoring function\relax }}{66}{algorithm.2}}
\newlabel{aistat:algo1}{{2}{66}{~~Learning a scoring function\relax }{algorithm.2}{}}
\newlabel{aistat:definition_sN}{{5.8}{66}{~~Learning a scoring function\relax }{equation.5.5.8}{}}
\newlabel{aistat:tk-omegak}{{5.9}{66}{Distributions with non compact support}{equation.5.5.9}{}}
\newlabel{aistat:tk1}{{5.10}{66}{Distributions with non compact support}{equation.5.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Unsuccessful mass-volume criterion optimization\relax }}{67}{figure.caption.17}}
\newlabel{aistat:algo-problem}{{5.3}{67}{Unsuccessful mass-volume criterion optimization\relax }{figure.caption.17}{}}
\newlabel{aistat:fondineq}{{5.11}{67}{Distributions with non compact support}{equation.5.5.11}{}}
\newlabel{aistat:tk}{{5.12}{67}{Distributions with non compact support}{equation.5.5.12}{}}
\newlabel{aistat:thmprinc}{{5.14}{67}{}{theorem.5.5.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Bias analysis}{68}{subsection.5.5.2}}
\newlabel{aistat:biais}{{5.5.2}{68}{Bias analysis}{subsection.5.5.2}{}}
\newlabel{aistat:thmprincbiais}{{5.15}{68}{}{theorem.5.5.15}{}}
\citation{CLEM13}
\newlabel{aistat:propbiais}{{5.17}{69}{}{theorem.5.5.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Simulation examples}{69}{section.5.6}}
\newlabel{aistat:sec:simul}{{5.6}{69}{Simulation examples}{section.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Optimal and realized EM curves\relax }}{70}{figure.caption.18}}
\newlabel{aistat:EMMS}{{5.4}{70}{Optimal and realized EM curves\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Zoom near 0 \nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\relax }}{70}{figure.caption.18}}
\newlabel{aistat:EMMSzoom}{{5.5}{70}{Zoom near 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces $EM_\mathcal  {G}$ for different $l$\relax }}{70}{figure.caption.19}}
\newlabel{aistat:EMGEM}{{5.6}{70}{$EM_\mathcal {G}$ for different $l$\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces density and scoring functions\relax }}{71}{figure.caption.20}}
\newlabel{aistat:scoring3D}{{5.7}{71}{density and scoring functions\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Detailed Proofs}{71}{section.5.7}}
\newlabel{aistat:sec:detailed_proofs}{{5.7}{71}{Detailed Proofs}{section.5.7}{}}
\citation{Federer1969}
\newlabel{aistat:lemmeMs}{{5.18}{75}{}{theorem.5.7.18}{}}
\newlabel{aistat:lemmederive}{{5.19}{75}{}{theorem.5.7.19}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Accuracy on Extreme Regions}{77}{part.3}}
\citation{COLT15}
\citation{Huangphd}
\citation{Drees98}
\citation{Embrechts2000}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\newlabel{part:vect}{{III}{79}{}{part.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Learning the dependence structure of rare events: a non-asymptotic study}{79}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{colt}{{6}{79}{Learning the dependence structure of rare events: a non-asymptotic study}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{79}{section.6.1}}
\newlabel{colt:sec:intro}{{6.1}{79}{Introduction}{section.6.1}{}}
\citation{Resnick2007}
\citation{Resnick1987}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Background on the stable tail dependence function}{80}{section.6.2}}
\newlabel{colt:sec:background}{{6.2}{80}{Background on the stable tail dependence function}{section.6.2}{}}
\newlabel{colt:stdf1}{{6.1}{80}{Background on the stable tail dependence function}{equation.6.2.1}{}}
\citation{Falk94}
\citation{Falk94}
\citation{Vapnik74}
\citation{Anthony93}
\citation{Bousquet04}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}A VC-type inequality adapted to the study of low probability regions}{81}{section.6.3}}
\newlabel{colt:sec:concentration}{{6.3}{81}{A VC-type inequality adapted to the study of low probability regions}{section.6.3}{}}
\newlabel{colt:thm-princ}{{6.1}{81}{}{theorem.6.3.1}{}}
\newlabel{colt:thm-princ-ineq}{{6.2}{81}{}{equation.6.3.2}{}}
\citation{Bousquet04}
\newlabel{colt:normalize-vc}{{6.3}{82}{}{equation.6.3.3}{}}
\newlabel{colt:rk:interpretation}{{6.4}{82}{}{theorem.6.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Classification of Extremes}{82}{theorem.6.3.4}}
\newlabel{colt:prediction:rates}{{6.4}{83}{Classification of Extremes}{equation.6.3.4}{}}
\newlabel{rk:classif-details}{{6.5}{83}{}{theorem.6.3.5}{}}
\citation{Huangphd}
\citation{Qi97}
\citation{Drees98}
\citation{Einmahl2006}
\citation{Huangphd}
\citation{Drees98}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\citation{Devroye96}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}A bound on the STDF}{84}{section.6.4}}
\newlabel{colt:sec:stdf}{{6.4}{84}{A bound on the STDF}{section.6.4}{}}
\newlabel{colt:stdf}{{6.5}{84}{A bound on the STDF}{equation.6.4.5}{}}
\newlabel{colt:ln}{{6.6}{84}{A bound on the STDF}{equation.6.4.6}{}}
\citation{ELL2009}
\citation{Qi97}
\newlabel{colt:Qialt2}{{6.7}{85}{A bound on the STDF}{equation.6.4.7}{}}
\newlabel{colt:thm:l}{{6.6}{85}{}{theorem.6.4.6}{}}
\newlabel{colt:thm:l:ineq}{{6.8}{85}{}{equation.6.4.8}{}}
\newlabel{colt:ln-Fn}{{6.7}{85}{Link between $l_n$ and $\tilde F_n$}{theorem.6.4.7}{}}
\citation{Wellner78}
\newlabel{colt:Fn-tildeF}{{6.8}{86}{Uniform bound on $\tilde F_n$'s deviations}{theorem.6.4.8}{}}
\newlabel{colt:U-x}{{6.9}{86}{Bound on the order statistics of $\mb U$}{theorem.6.4.9}{}}
\newlabel{colt:eq-Wellner}{{6.9}{86}{Bound on the order statistics of $\mb U$}{equation.6.4.9}{}}
\citation{Qi97}
\newlabel{colt:unif_conv}{{6.10}{87}{A bound on the STDF}{equation.6.4.10}{}}
\citation{deHaan1996}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Discussion}{88}{section.6.5}}
\newlabel{colt:sec:conclusion}{{6.5}{88}{Discussion}{section.6.5}{}}
\citation{ARXIV16}
\citation{AISTAT16}
\citation{NIPSWORKSHOP15}
\citation{finkenstadt2003extreme,smith2003statistics}
\citation{Clifton2011,Lee2008}
\citation{Resnick1987}
\citation{BGTS04}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Sparse Representation of Multivariate Extremes}{91}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{jmva}{{7}{91}{Sparse Representation of Multivariate Extremes}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{91}{section.7.1}}
\newlabel{jmva:sec:intro}{{7.1}{91}{Introduction}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Context: multivariate extreme values in large dimension}{91}{subsection.7.1.1}}
\citation{coles1991modeling,fougeres2009models,cooley2010pairwise,sabourinNaveau2012}
\citation{Segers12Bernoulli,Drees98,Embrechts2000,Einmahl2012,dHF06}
\citation{Einmahl2001}
\citation{Einmahl2009}
\citation{stephenson2009high}
\citation{sabourinNaveau2012}
\citation{COLT15}
\citation{Einmahl2001}
\citation{Einmahl2009}
\citation{Roberts99}
\citation{Roberts2000}
\citation{Clifton2011}
\citation{Clifton2008}
\citation{Lee2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Application to Anomaly Detection}{93}{subsection.7.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Multivariate EVT Framework and Problem Statement}{94}{section.7.2}}
\newlabel{jmva:sec:framework}{{7.2}{94}{Multivariate EVT Framework and Problem Statement}{section.7.2}{}}
\newlabel{jmva:intro:assumption2}{{7.1}{94}{Multivariate EVT Framework and Problem Statement}{equation.7.2.1}{}}
\newlabel{jmva:intro:regvar}{{7.2}{94}{Multivariate EVT Framework and Problem Statement}{equation.7.2.2}{}}
\newlabel{jmva:eq:regularVariation}{{7.3}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.3}{}}
\newlabel{jmva:eq:pseudoPolar_change}{{7.4}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.4}{}}
\newlabel{jmva:mu-phi}{{7.5}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.5}{}}
\newlabel{jmva:eq:integratePhiLambda}{{7.6}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.6}{}}
\newlabel{jmva:eq:limitConditAngle}{{7.7}{95}{Multivariate EVT Framework and Problem Statement}{equation.7.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Statement of the Statistical Problem}{95}{subsection.7.2.1}}
\newlabel{jmva:sec:decomposMu}{{7.2.1}{95}{Statement of the Statistical Problem}{subsection.7.2.1}{}}
\newlabel{jmva:cone}{{7.8}{95}{Statement of the Statistical Problem}{equation.7.2.8}{}}
\newlabel{jmva:eq:decomp1}{{7.9}{96}{Statement of the Statistical Problem}{equation.7.2.9}{}}
\newlabel{jmva:eq:epsilon_Rectangle}{{7.10}{96}{Statement of the Statistical Problem}{equation.7.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Truncated cones in 3D\relax }}{96}{figure.7.1}}
\newlabel{jmva:fig:3Dcones}{{7.1}{96}{Truncated cones in 3D\relax }{figure.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Truncated $\epsilon $-rectangles in 2D\relax }}{96}{figure.7.2}}
\newlabel{jmva:2Dcones}{{7.2}{96}{Truncated $\epsilon $-rectangles in 2D\relax }{figure.7.2}{}}
\newlabel{jmva:lem:limit_muCalphaEps}{{7.1}{96}{}{theorem.7.2.1}{}}
\newlabel{jmva:rk_approx_mu_n}{{7.2}{97}{}{theorem.7.2.2}{}}
\citation{Einmahl2009}
\newlabel{jmva:eq:representation_M}{{7.13}{98}{Statement of the Statistical Problem}{equation.7.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Regularity Assumptions}{98}{subsection.7.2.2}}
\newlabel{jmva:sec:RegularAssumptions}{{7.2.2}{98}{Regularity Assumptions}{subsection.7.2.2}{}}
\newlabel{jmva:hypo:continuous_margins}{{1}{98}{}{assumption.1}{}}
\newlabel{jmva:hypo:continuousMu}{{2}{98}{}{assumption.2}{}}
\newlabel{jmva:lem:continuousPhi}{{7.3}{98}{}{theorem.7.2.3}{}}
\newlabel{jmva:eq:decomposePhi}{{7.15}{99}{Regularity Assumptions}{equation.7.2.15}{}}
\newlabel{jmva:eq:supDensity}{{7.16}{99}{Regularity Assumptions}{equation.7.2.16}{}}
\newlabel{jmva:hypo:abs_continuousPhi}{{3}{99}{}{assumption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}A non-parametric estimator of the subcones' mass : definition and preliminary results}{99}{section.7.3}}
\newlabel{jmva:sec:estimation}{{7.3}{99}{A non-parametric estimator of the subcones' mass : definition and preliminary results}{section.7.3}{}}
\citation{Einmahl2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}A natural empirical version of the exponent measure mu}{100}{subsection.7.3.1}}
\newlabel{jmva:sec:classicEstimators}{{7.3.1}{100}{A natural empirical version of the exponent measure mu}{subsection.7.3.1}{}}
\newlabel{jmva:def:transform}{{7.17}{100}{A natural empirical version of the exponent measure mu}{equation.7.3.17}{}}
\newlabel{jmva:mu_n}{{7.18}{100}{A natural empirical version of the exponent measure mu}{equation.7.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Accounting for the non asymptotic nature of data: epsilon-thickening.}{100}{subsection.7.3.2}}
\newlabel{jmva:heuristic_mu_n}{{7.19}{100}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Estimation procedure\relax }}{101}{figure.caption.21}}
\newlabel{jmva:estimation_rect}{{7.3}{101}{Estimation procedure\relax }{figure.caption.21}{}}
\newlabel{jmva:eq:interprete_mun_Pcondit}{{7.20}{101}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.20}{}}
\newlabel{jmva:error_decomp}{{7.21}{101}{Accounting for the non asymptotic nature of data: epsilon-thickening}{equation.7.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Preliminaries: uniform approximation over a VC-class of rectangles}{101}{subsection.7.3.3}}
\newlabel{jmva:sec:rectangles}{{7.3.3}{101}{Preliminaries: uniform approximation over a VC-class of rectangles}{subsection.7.3.3}{}}
\citation{Huangphd}
\citation{Qi97}
\citation{Drees98}
\citation{Einmahl2006}
\citation{COLT15}
\citation{Huangphd}
\citation{Drees98}
\citation{Embrechts2000}
\citation{dHF06}
\citation{Qi97}
\citation{Einmahl2012}
\citation{COLT15}
\newlabel{jmva:stdf}{{7.22}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.22}{}}
\newlabel{jmva:def:tildeF}{{7.23}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.23}{}}
\newlabel{jmva:empir-Stdf}{{7.24}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.24}{}}
\newlabel{jmva:lem:equivalence}{{7.5}{102}{}{theorem.7.3.5}{}}
\newlabel{jmva:empir-Stdf2}{{7.25}{102}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.25}{}}
\newlabel{jmva:set-R}{{7.26}{103}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.26}{}}
\newlabel{jmva:g-alpha}{{7.27}{103}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.27}{}}
\newlabel{jmva:F-tilde-alpha}{{7.28}{103}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.28}{}}
\newlabel{jmva:g_with_mu}{{7.29}{103}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.29}{}}
\newlabel{jmva:lem:g-alpha}{{7.6}{103}{}{theorem.7.3.6}{}}
\citation{COLT15}
\citation{COLT15}
\citation{COLT15}
\newlabel{jmva:def:gn}{{7.30}{104}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.30}{}}
\newlabel{jmva:eq:gn2}{{7.31}{104}{Preliminaries: uniform approximation over a VC-class of rectangles}{equation.7.3.31}{}}
\newlabel{jmva:prop:g}{{7.8}{104}{}{theorem.7.3.8}{}}
\newlabel{jmva:ineq:g}{{7.32}{104}{}{equation.7.3.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Bounding empirical deviations over thickened rectangles}{105}{subsection.7.3.4}}
\newlabel{jmva:sec:boundErrorEpsilonCones}{{7.3.4}{105}{Bounding empirical deviations over thickened rectangles}{subsection.7.3.4}{}}
\newlabel{jmva:cor:mu_n-mu}{{7.9}{105}{}{theorem.7.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.5}Bounding the bias induced by thickened rectangles}{105}{subsection.7.3.5}}
\newlabel{jmva:sec:boundMuEpsilonCones}{{7.3.5}{105}{Bounding the bias induced by thickened rectangles}{subsection.7.3.5}{}}
\newlabel{jmva:eq:epsilon_Cone}{{7.34}{105}{Bounding the bias induced by thickened rectangles}{equation.7.3.34}{}}
\newlabel{jmva:eq:epsilon_Sphere}{{7.35}{105}{Bounding the bias induced by thickened rectangles}{equation.7.3.35}{}}
\newlabel{jmva:eq:approx_Recctangle}{{7.36}{105}{Bounding the bias induced by thickened rectangles}{equation.7.3.36}{}}
\citation{ELL2009}
\newlabel{jmva:lemma_simplex}{{7.10}{106}{}{theorem.7.3.10}{}}
\newlabel{jmva:prop_simplex}{{7.11}{106}{}{theorem.7.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.6}Main result}{106}{subsection.7.3.6}}
\newlabel{jmva:thm-princ}{{7.12}{106}{}{theorem.7.3.12}{}}
\newlabel{jmva:eq:bias}{{7.37}{106}{Main result}{equation.7.3.37}{}}
\newlabel{jmva:rk:threshold}{{7.13}{107}{}{theorem.7.3.13}{}}
\newlabel{jmva:rk:optim}{{7.14}{107}{}{theorem.7.3.14}{}}
\newlabel{jmva:eq:optim_pb}{{7.38}{108}{}{equation.7.3.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Application to Anomaly Detection }{108}{section.7.4}}
\newlabel{jmva:sec:appliAD}{{7.4}{108}{Application to Anomaly Detection }{section.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Extremes and Anomaly Detection.}{108}{subsection.7.4.1}}
\newlabel{jmva:sec:appliADBackground}{{7.4.1}{108}{Extremes and Anomaly Detection}{subsection.7.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}DAMEX Algorithm: Detecting Anomalies among Multivariate Extremes}{109}{subsection.7.4.2}}
\newlabel{jmva:sec:algo}{{7.4.2}{109}{DAMEX Algorithm: Detecting Anomalies among Multivariate Extremes}{subsection.7.4.2}{}}
\newlabel{jmva:heuristic_mu_n2}{{7.39}{110}{DAMEX Algorithm: Detecting Anomalies among Multivariate Extremes}{equation.7.4.39}{}}
\newlabel{jmva:eq:angularscoring}{{7.40}{110}{DAMEX Algorithm: Detecting Anomalies among Multivariate Extremes}{equation.7.4.40}{}}
\newlabel{jmva:def:scoring}{{7.41}{110}{DAMEX Algorithm: Detecting Anomalies among Multivariate Extremes}{equation.7.4.41}{}}
\citation{AISTAT16}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Level sets of $s_n$ on simulated 2D data\relax }}{111}{figure.caption.22}}
\newlabel{jmva:DAMEX-2D}{{7.4}{111}{Level sets of $s_n$ on simulated 2D data\relax }{figure.caption.22}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces DAMEX\relax }}{111}{algorithm.3}}
\newlabel{jmva:DAMEX-algo}{{3}{111}{DAMEX\relax }{algorithm.3}{}}
\newlabel{jmva:phi_n}{{7.42}{111}{DAMEX\relax }{equation.7.4.42}{}}
\citation{Coles2001}
\citation{Tawn90}
\citation{Stephenson2003}
\newlabel{jmva:rk_param_interpretation}{{7.15}{112}{}{theorem.7.4.15}{}}
\newlabel{jmva:rk_param_choice}{{7.16}{112}{}{theorem.7.4.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Experimental results}{112}{section.7.5}}
\newlabel{jmva:sec:experiments}{{7.5}{112}{Experimental results}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Recovering the support of the dependence structure of generated data}{112}{subsection.7.5.1}}
\citation{Liu2008}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Support recovering on simulated data\relax }}{113}{table.caption.23}}
\newlabel{jmva:table:logevd}{{7.1}{113}{Support recovering on simulated data\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Sparse structure of extremes (wave data)}{113}{subsection.7.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces sub-cone dimensions of wave data\relax }}{113}{figure.caption.24}}
\newlabel{jmva:fig:wavedata-dim}{{7.5}{113}{sub-cone dimensions of wave data\relax }{figure.caption.24}{}}
\citation{Lichman2013}
\citation{Lichman2013}
\citation{KDD99}
\citation{Tavallaee2009}
\citation{Lippmann2000}
\citation{Yamanishi2000}
\citation{Eskin2002}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Total number of sub-cones of wave data\relax }}{114}{table.caption.25}}
\newlabel{jmva:fig:wavedata-nb-faces}{{7.2}{114}{Total number of sub-cones of wave data\relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Application to Anomaly Detection on real-world data sets}{114}{subsection.7.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Datasets characteristics\relax }}{115}{table.caption.26}}
\newlabel{jmva:table:data}{{7.3}{115}{Datasets characteristics\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Results on extreme regions with standard parameters $(k,\epsilon ) = (n^{1/2}, 0.01)$\relax }}{115}{table.caption.27}}
\newlabel{jmva:table:results-dbad+iforest-01}{{7.4}{115}{Results on extreme regions with standard parameters $(k,\epsilon ) = (n^{1/2}, 0.01)$\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Results on extreme regions with lower $\epsilon =0.1$\relax }}{115}{table.caption.28}}
\newlabel{jmva:table:results-dbad+iforest-1}{{7.5}{115}{Results on extreme regions with lower $\epsilon =0.1$\relax }{table.caption.28}{}}
\citation{Fougeres2015}
\citation{Beirlant2015}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces shuttle, default parameters\relax }}{116}{figure.caption.29}}
\newlabel{jmva:fig:shuttle}{{7.6}{116}{shuttle, default parameters\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces SF dataset, default parameters\relax }}{116}{figure.caption.30}}
\newlabel{jmva:fig:SF}{{7.7}{116}{SF dataset, default parameters\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces SF dataset, larger $\epsilon $\relax }}{117}{figure.caption.31}}
\newlabel{jmva:fig:SF_1}{{7.8}{117}{SF dataset, larger $\epsilon $\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces SA dataset, default parameters\relax }}{117}{figure.caption.32}}
\newlabel{jmva:fig:SA}{{7.9}{117}{SA dataset, default parameters\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Conclusion}{117}{section.7.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces forestcover dataset, default parameters\relax }}{118}{figure.caption.33}}
\newlabel{jmva:fig:forestcover}{{7.10}{118}{forestcover dataset, default parameters\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces http dataset, default parameters\relax }}{118}{figure.caption.34}}
\newlabel{jmva:fig:http}{{7.11}{118}{http dataset, default parameters\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Technical proofs}{119}{section.7.7}}
\newlabel{jmva:appendix_proof}{{7.7}{119}{Technical proofs}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Proof of Lemma \ref  {jmva:lem:equivalence}}{119}{subsection.7.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Proof of Lemma \ref  {jmva:lem:g-alpha}}{119}{subsection.7.7.2}}
\citation{COLT15}
\citation{COLT15}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Proof of Proposition \ref  {jmva:prop:g}}{120}{subsection.7.7.3}}
\newlabel{jmva:Qialt2}{{7.43}{120}{Proof of Proposition \ref {jmva:prop:g}}{equation.7.7.43}{}}
\citation{COLT15}
\citation{COLT15}
\citation{COLT15}
\newlabel{jmva:cornerstone-extension}{{7.44}{121}{Proof of Proposition \ref {jmva:prop:g}}{equation.7.7.44}{}}
\newlabel{jmva:def:Fn}{{7.45}{121}{Proof of Proposition \ref {jmva:prop:g}}{equation.7.7.45}{}}
\citation{COLT15}
\newlabel{jmva:lem:gn-Fn}{{7.20}{122}{Link between $g_{n, \alpha , \beta }$ and $\tilde F_{n, \alpha , \beta }$}{theorem.7.7.20}{}}
\newlabel{jmva:lem:Fn-tildeF}{{7.21}{122}{Uniform bound on $\tilde F_{n, \alpha , \beta }$'s deviations}{theorem.7.7.21}{}}
\newlabel{jmva:rk:lem:Fn-tildeF_generalized}{{7.22}{122}{}{theorem.7.7.22}{}}
\newlabel{jmva:lem:U-x}{{7.23}{122}{Bound on the order statistics of $\mb U$}{theorem.7.7.23}{}}
\newlabel{jmva:lem:eq-Wellner}{{7.46}{122}{Bound on the order statistics of $\mb U$}{equation.7.7.46}{}}
\newlabel{jmva:proof_decomp}{{7.47}{123}{Proof of Proposition \ref {jmva:prop:g}}{equation.7.7.47}{}}
\citation{Qi97}
\newlabel{jmva:rk:bias}{{7.24}{124}{}{theorem.7.7.24}{}}
\newlabel{jmva:unif_conv}{{7.48}{124}{}{equation.7.7.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}Proof of Lemma \ref  {jmva:lemma_simplex}}{124}{subsection.7.7.4}}
\newlabel{jmva:pr:decompPhi}{{7.49}{125}{Proof of Lemma \ref {jmva:lemma_simplex}}{equation.7.7.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}Proof of Remark\nobreakspace  {}\ref  {jmva:rk:optim}}{126}{subsection.7.7.5}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Efficient heuristic approaches}{127}{part.4}}
\citation{ICMLworkshop16}
\citation{NIPS16evaluation}
\citation{Roberts99}
\citation{Barnett94,Eskin2000}
\citation{Scholkopf2001,Scott2006,Breunig2000LOF}
\citation{Shyu2003,Aggarwal2001}
\citation{Liu2008}
\citation{Hodge2004survey,Chandola2009survey,Patcha2007survey,Markou2003survey}
\newlabel{part:heuristic}{{IV}{129}{Proof of Remark~\ref {jmva:rk:optim}}{part.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}How to Evaluate the Quality of Anomaly Detection Algorithms?}{129}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{8}{129}{How to Evaluate the Quality of Anomaly Detection Algorithms?}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{129}{section.8.1}}
\newlabel{evaluation:sec:intro}{{8.1}{129}{Introduction}{section.8.1}{}}
\citation{Vert06thesis}
\citation{Scholkopf2001}
\citation{Cortes1995,Shawe2004}
\@writefile{toc}{\contentsline {paragraph}{What is a scoring function?}{130}{section.8.1}}
\@writefile{toc}{\contentsline {paragraph}{How to know if a scoring function is good?}{130}{section.8.1}}
\citation{CLEM13,CLEM14}
\citation{AISTAT15}
\citation{Thomas2015}
\citation{CLEM13,AISTAT15}
\citation{CLEM13,AISTAT15}
\citation{CLEM13,AISTAT15}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Mass-Volume and Excess-Mass based criteria}{131}{section.8.2}}
\newlabel{evaluation:background}{{8.2}{131}{Mass-Volume and Excess-Mass based criteria}{section.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Preliminaries}{131}{subsection.8.2.1}}
\newlabel{evaluation:MV-def}{{8.1}{131}{Preliminaries}{equation.8.2.1}{}}
\newlabel{evaluation:EM-def}{{8.2}{131}{Preliminaries}{equation.8.2.2}{}}
\citation{Polonik97}
\citation{Polonik95}
\newlabel{evaluation:eq:MV}{{8.3}{132}{Preliminaries}{equation.8.2.3}{}}
\newlabel{evaluation:eq:EM}{{8.4}{132}{Preliminaries}{equation.8.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }}{132}{figure.caption.35}}
\newlabel{evaluation:aistat:MVcurve}{{8.1}{132}{Comparison between $MV^*(\alpha )$ and $EM^*(t)$\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Numerical unsupervised criteria}{132}{subsection.8.2.2}}
\citation{CLEM14}
\newlabel{evaluation:MV-def-emp}{{8.5}{133}{Numerical unsupervised criteria}{equation.8.2.5}{}}
\newlabel{evaluation:EM-def-emp}{{8.6}{133}{Numerical unsupervised criteria}{equation.8.2.6}{}}
\newlabel{evaluation:eq:standard_emp_EM}{{8.7}{133}{Numerical unsupervised criteria}{equation.8.2.7}{}}
\newlabel{evaluation:eq:standard_emp_MV}{{8.8}{133}{Numerical unsupervised criteria}{equation.8.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Scaling with dimension}{133}{section.8.3}}
\newlabel{evaluation:scaling-dim}{{8.3}{133}{Scaling with dimension}{section.8.3}{}}
\citation{Scholkopf2001}
\citation{Liu2008}
\citation{Breunig2000LOF}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces \nobreakspace  {}\nobreakspace  {}Evaluate anomaly detection algorithms on high dimensional data\relax }}{134}{algorithm.4}}
\newlabel{evaluation:algo:EMMV}{{4}{134}{~~Evaluate anomaly detection algorithms on high dimensional data\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Benchmarks}{134}{section.8.4}}
\newlabel{evaluation:sec:benchmarks}{{8.4}{134}{Benchmarks}{section.8.4}{}}
\citation{Lichman2013}
\citation{KDD99,Tavallaee2009}
\citation{Yamanishi2000}
\citation{sklearn2011}
\citation{Liu2008}
\citation{Liu2008}
\citation{Schubert2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Datasets description}{135}{subsection.8.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Original Datasets characteristics\relax }}{135}{table.caption.36}}
\newlabel{evaluation:table:data}{{8.1}{135}{Original Datasets characteristics\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Results}{136}{subsection.8.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces Results for the novelty detection setting. ROC, PR, EM, MV often do agree on which algorithm is the best (in bold), which algorithm is the worse (underlined) on some fixed datasets. When they do not agree, it is often because ROC and PR themselves do not, meaning that the ranking is not clear.\relax }}{136}{table.caption.37}}
\newlabel{evaluation:table:results-semisupervised}{{8.2}{136}{Results for the novelty detection setting. ROC, PR, EM, MV often do agree on which algorithm is the best (in bold), which algorithm is the worse (underlined) on some fixed datasets. When they do not agree, it is often because ROC and PR themselves do not, meaning that the ranking is not clear.\relax }{table.caption.37}{}}
\newlabel{evaluation:eq:pairs}{{8.9}{136}{Results}{equation.8.4.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.3}{\ignorespaces Results for the unsupervised setting still remain good: one can see that ROC, PR, EM, MV often do agree on which algorithm is the best (in bold), which algorithm is the worse (underlined) on some fixed datasets. When they do not agree, it is often because ROC and PR themselves do not, meaning that the ranking is not clear.\relax }}{137}{table.caption.38}}
\newlabel{evaluation:table:results-unsupervised}{{8.3}{137}{Results for the unsupervised setting still remain good: one can see that ROC, PR, EM, MV often do agree on which algorithm is the best (in bold), which algorithm is the worse (underlined) on some fixed datasets. When they do not agree, it is often because ROC and PR themselves do not, meaning that the ranking is not clear.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces MV and EM curves for adult dataset (novelty detection framework). Both in terms of EM and MV curves, iForest is found to perform better than OCSVM, which is itself found to perform better than LOF. Comparing to Table\nobreakspace  {}\ref  {evaluation:table:results-semisupervised}, ROC and PR AUCs give the same ranking (iForest on adult $\succ $ OCSVM on adult $\succ $ LOF on adult). The 3 pairwise comparisons (iForest on adult, LOF on adult), (OCSVM on adult, LOF on adult) and (OCSVM on adult, iForest on adult) are then similarly ordered by EM, PR, MV and EM criteria.\relax }}{137}{figure.caption.39}}
\newlabel{evaluation:mv_em_adult}{{8.2}{137}{MV and EM curves for adult dataset (novelty detection framework). Both in terms of EM and MV curves, iForest is found to perform better than OCSVM, which is itself found to perform better than LOF. Comparing to Table~\ref {evaluation:table:results-semisupervised}, ROC and PR AUCs give the same ranking (iForest on adult $\succ $ OCSVM on adult $\succ $ LOF on adult). The 3 pairwise comparisons (iForest on adult, LOF on adult), (OCSVM on adult, LOF on adult) and (OCSVM on adult, iForest on adult) are then similarly ordered by EM, PR, MV and EM criteria.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Conclusion}{139}{section.8.5}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Further material on the experiments}{140}{section.8.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces ROC and PR curves for Isolation Forest (novelty detection framework)\relax }}{140}{figure.caption.40}}
\newlabel{evaluation:fig:iforest_roc_pr}{{8.3}{140}{ROC and PR curves for Isolation Forest (novelty detection framework)\relax }{figure.caption.40}{}}
\newlabel{evaluation:fig:iforest_roc_pr_unsupervised}{{\caption@xref {evaluation:fig:iforest_roc_pr_unsupervised}{ on input line 511}}{140}{Further material on the experiments}{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces ROC and PR curves for Isolation Forest (unsupervised framework)\relax }}{140}{figure.caption.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces ROC and PR curves for One Class SVM (novelty detection framework)\relax }}{141}{figure.caption.42}}
\newlabel{evaluation:fig:ocsvm_roc_pr}{{8.5}{141}{ROC and PR curves for One Class SVM (novelty detection framework)\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces ROC and PR curves for One Class SVM (unsupervised framework)\relax }}{141}{figure.caption.43}}
\newlabel{evaluation:fig:ocsvm_roc_pr_unsupervised}{{8.6}{141}{ROC and PR curves for One Class SVM (unsupervised framework)\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces ROC and PR curves for Local Outlier Factor (novelty detection framework)\relax }}{142}{figure.caption.44}}
\newlabel{evaluation:fig:lof_roc_pr}{{8.7}{142}{ROC and PR curves for Local Outlier Factor (novelty detection framework)\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces ROC and PR curves for Local Outlier Factor (unsupervised framework)\relax }}{142}{figure.caption.45}}
\newlabel{evaluation:fig:lof_roc_pr_unsupervised}{{8.8}{142}{ROC and PR curves for Local Outlier Factor (unsupervised framework)\relax }{figure.caption.45}{}}
\newlabel{evaluation:mv_em_http}{{\caption@xref {evaluation:mv_em_http}{ on input line 548}}{143}{Further material on the experiments}{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces MV and EM curves for http dataset (novelty detection framework)\relax }}{143}{figure.caption.46}}
\newlabel{evaluation:mv_em_http_unsupervised}{{\caption@xref {evaluation:mv_em_http_unsupervised}{ on input line 554}}{143}{Further material on the experiments}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces MV and EM curves for http dataset (unsupervised framework)\relax }}{143}{figure.caption.47}}
\newlabel{evaluation:mv_em_pima}{{\caption@xref {evaluation:mv_em_pima}{ on input line 574}}{144}{Further material on the experiments}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces MV and EM curves for pima dataset (novelty detection framework)\relax }}{144}{figure.caption.48}}
\newlabel{evaluation:mv_em_pima_unsupervised}{{\caption@xref {evaluation:mv_em_pima_unsupervised}{ on input line 580}}{144}{Further material on the experiments}{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces MV and EM curves for pima dataset (unsupervised framework)\relax }}{144}{figure.caption.49}}
\newlabel{evaluation:mv_em_smtp}{{\caption@xref {evaluation:mv_em_smtp}{ on input line 587}}{145}{Further material on the experiments}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces MV and EM curves for smtp dataset (novelty detection framework)\relax }}{145}{figure.caption.50}}
\newlabel{evaluation:mv_em_smtp_unsupervised}{{\caption@xref {evaluation:mv_em_smtp_unsupervised}{ on input line 593}}{145}{Further material on the experiments}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces MV and EM curves for smtp dataset (unsupervised framework)\relax }}{145}{figure.caption.51}}
\newlabel{evaluation:mv_em_wilt}{{\caption@xref {evaluation:mv_em_wilt}{ on input line 600}}{146}{Further material on the experiments}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces MV and EM curves for wilt dataset (novelty detection framework)\relax }}{146}{figure.caption.52}}
\newlabel{evaluation:mv_em_wilt_unsupervised}{{\caption@xref {evaluation:mv_em_wilt_unsupervised}{ on input line 606}}{146}{Further material on the experiments}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces MV and EM curves for wilt dataset (unsupervised framework)\relax }}{146}{figure.caption.53}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces MV and EM curves for adult dataset (novelty detection framework).\relax }}{147}{figure.caption.54}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces MV and EM curves for adult dataset (unsupervised framework)\relax }}{147}{figure.caption.55}}
\newlabel{evaluation:mv_em_adult_unsupervised}{{8.18}{147}{MV and EM curves for adult dataset (unsupervised framework)\relax }{figure.caption.55}{}}
\citation{OCRF16}
\citation{Barnett94,Eskin2000}
\citation{Scholkopf2001,Scott2006,Breunig2000LOF,Quinn2014}
\citation{Shyu2003,Aggarwal2001}
\citation{Liu2008,Shi2012}
\citation{Hodge2004survey,Chandola2009survey,Patcha2007survey,Markou2003survey}
\citation{Scholkopf2001}
\citation{Cortes1995,Shawe2004}
\citation{Quinn2014}
\citation{Sugiyama2010}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}One Class Splitting Criteria for Random Forests}{149}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ocrf}{{9}{149}{One Class Splitting Criteria for Random Forests}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction}{149}{section.9.1}}
\newlabel{ocrf:sec:intro}{{9.1}{149}{Introduction}{section.9.1}{}}
\citation{Breiman2001}
\citation{Freund1996}
\citation{Svetnik2003,Diaz2006,Genuer2010}
\citation{Genuer2008,Biau2008,Louppe2014,Biau2016}
\citation{Desir12,Liu2008,Shi2012}
\citation{Liu2008}
\citation{Geurts2006}
\citation{Desir12,Shi2012}
\citation{Desir12}
\citation{Shi2012}
\citation{Tax2002}
\citation{Scott2006}
\citation{Polonik97,Einmahl1992}
\citation{Blanchard2004}
\citation{CLEM14}
\citation{Clemencon2009Tree}
\citation{Scott2006,CLEM14}
\citation{CLEM14}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Background on decision trees}{151}{section.9.2}}
\newlabel{ocrf:sec:background}{{9.2}{151}{Background on decision trees}{section.9.2}{}}
\citation{Gini1912}
\citation{Shannon2001}
\citation{Geurts2006}
\newlabel{ocrf:eq:impurity_measure_decrease}{{9.1}{152}{Background on decision trees}{equation.9.2.1}{}}
\newlabel{ocrf:eq:gini}{{9.2}{152}{Background on decision trees}{equation.9.2.2}{}}
\newlabel{ocrf:eq:two_class_proxy}{{9.3}{152}{Background on decision trees}{equation.9.2.3}{}}
\newlabel{ocrf:tc_proxy}{{9.4}{152}{Background on decision trees}{equation.9.2.4}{}}
\citation{Desir12,Shi2012}
\citation{Flach2003}
\citation{Scott2009}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Adaptation to the one-class setting}{153}{section.9.3}}
\newlabel{ocrf:sec:one-class}{{9.3}{153}{Adaptation to the one-class setting}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}One-class splitting criterion}{153}{subsection.9.3.1}}
\newlabel{sec:one-class-crit}{{9.3.1}{153}{One-class splitting criterion}{subsection.9.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Naive approach.}{153}{subsection.9.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Adaptive approach.}{153}{theorem.9.3.1}}
\citation{Liu2008}
\citation{Geurts2006}
\newlabel{ocrf:oc_proxy_ad2}{{9.5}{154}{Adaptive approach}{equation.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Prediction: a majority vote with one single candidate?}{154}{subsection.9.3.2}}
\newlabel{ocrf:sec:prediction}{{9.3.2}{154}{Prediction: a majority vote with one single candidate?}{subsection.9.3.2}{}}
\citation{Liu2008}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Outliers assumed distribution $G$ in the naive and adaptive approach. In the naive approach, $G$ does not depends on the tree and is constant on the input space. In the adaptive approach the distribution depends on the inlier distribution $F$ through the tree. The outliers density is constant and equal to the average of $F$ on each node before splitting it. Note that $G$ is tightly concentrated around the inliers, as required in high dimension. \relax }}{155}{figure.caption.56}}
\newlabel{ocrf:fig:outlier_density}{{9.1}{155}{Outliers assumed distribution $G$ in the naive and adaptive approach. In the naive approach, $G$ does not depends on the tree and is constant on the input space. In the adaptive approach the distribution depends on the inlier distribution $F$ through the tree. The outliers density is constant and equal to the average of $F$ on each node before splitting it. Note that $G$ is tightly concentrated around the inliers, as required in high dimension. \relax }{figure.caption.56}{}}
\newlabel{ocrf:eq:scoring3}{{9.6}{155}{Prediction: a majority vote with one single candidate?}{equation.9.3.6}{}}
\citation{Breiman2001}
\citation{Ho1998,Panov2007}
\citation{Amit1997}
\citation{Liu2008}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces  The left part of this figure represents the dataset under study and the underlying density. After some splits on this initial node $\mathcal  {X}$, let us consider the node $\mathcal  {X}_t$ illustrated in the right part of this figure: without the proposed adaptive approach, the class ratio $\gamma _t$ becomes too small and leads to poor splits (all the data are in the `normal side' of the split, which thus does not discriminate at all). Contrariwise, setting $\gamma $ to one, \textit  {i.e.} using our adaptive approach, is far preferable. Note that a given $\gamma $ corresponds to a level set $t_{\gamma }$.\relax }}{156}{figure.caption.57}}
\newlabel{ocrf:fig:split_alpha}{{9.2}{156}{The left part of this figure represents the dataset under study and the underlying density. After some splits on this initial node $\mathcal {X}$, let us consider the node $\mathcal {X}_t$ illustrated in the right part of this figure: without the proposed adaptive approach, the class ratio $\gamma _t$ becomes too small and leads to poor splits (all the data are in the `normal side' of the split, which thus does not discriminate at all). Contrariwise, setting $\gamma $ to one, \textit {i.e.} using our adaptive approach, is far preferable. Note that a given $\gamma $ corresponds to a level set $t_{\gamma }$.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}OneClassRF: a Generic One-Class Random Forest algorithm}{156}{subsection.9.3.3}}
\newlabel{ocrf:rk:gamma}{{9.5}{156}{}{theorem.9.3.5}{}}
\citation{Breiman2001}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces OneClassRF with one tree: level-sets of the scoring function\relax }}{157}{figure.caption.58}}
\newlabel{ocrf:fig:oneclassrf}{{9.3}{157}{OneClassRF with one tree: level-sets of the scoring function\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Illustration of the standard splitting criterion on two modes when the proportion $\gamma $ varies.\relax }}{157}{figure.caption.59}}
\newlabel{ocrf:fig:split_alpha_2}{{9.4}{157}{Illustration of the standard splitting criterion on two modes when the proportion $\gamma $ varies.\relax }{figure.caption.59}{}}
\citation{Liu2008}
\citation{Desir12}
\citation{Scholkopf2001}
\citation{Breunig2000LOF}
\citation{Bay2003}
\citation{Quinn2014}
\citation{Shi2012}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Benchmarks}{158}{section.9.4}}
\newlabel{ocrf:sec:benchmark}{{9.4}{158}{Benchmarks}{section.9.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Original Datasets characteristics\relax }}{158}{table.caption.60}}
\newlabel{ocrf:table:data}{{9.1}{158}{Original Datasets characteristics\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Results for the novelty detection setting (semi-supervised framework). The table reports AUC ROC and AUC PR scores (higher is better) for each algorithms. The training time of each algorithm has been limited (for each experiment among the 10 performed for each dataset) to 30 minutes, where `NA' indicates that the algorithm could not finish training within the allowed time limit. In average on all the datasets, our proposed algorithm `OneClassRF' achieves both best AUC ROC and AUC PR scores (with LSAD for AUC ROC). It also achieves the lowest cumulative training time.\relax }}{158}{table.caption.61}}
\newlabel{ocrf:table:results-semisupervised}{{9.2}{158}{Results for the novelty detection setting (semi-supervised framework). The table reports AUC ROC and AUC PR scores (higher is better) for each algorithms. The training time of each algorithm has been limited (for each experiment among the 10 performed for each dataset) to 30 minutes, where `NA' indicates that the algorithm could not finish training within the allowed time limit. In average on all the datasets, our proposed algorithm `OneClassRF' achieves both best AUC ROC and AUC PR scores (with LSAD for AUC ROC). It also achieves the lowest cumulative training time.\relax }{table.caption.61}{}}
\citation{Liu2008}
\citation{Liu2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Default parameters of OneClassRF.}{159}{subsection.9.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Hyper-Parameters of tested algorithms}{159}{subsection.9.4.2}}
\citation{Lichman2013}
\citation{KDD99,Tavallaee2009}
\citation{Yamanishi2000}
\citation{sklearn2011}
\citation{Liu2008}
\citation{Liu2008}
\citation{Schubert2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Description of the datasets}{160}{subsection.9.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Results}{160}{subsection.9.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Performances of the algorithms on each dataset in the novelty detection framework: ROC AUCs are displayed on the top, Precision-Recall AUCs in the middle and training times\footnotemark on the bottom, for each dataset and algorithm. The $x$-axis represents the datasets.\relax }}{161}{figure.caption.62}}
\newlabel{ocrf:figresultssemisupervised}{{9.5}{161}{Performances of the algorithms on each dataset in the novelty detection framework: ROC AUCs are displayed on the top, Precision-Recall AUCs in the middle and training times\protect \footnotemark on the bottom, for each dataset and algorithm. The $x$-axis represents the datasets.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Theoretical justification for the one-class splitting criterion}{161}{section.9.5}}
\newlabel{sec:ocrf:theory}{{9.5}{161}{Theoretical justification for the one-class splitting criterion}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Underlying model}{161}{subsection.9.5.1}}
\newlabel{ocrf:sec:model}{{9.5.1}{161}{Underlying model}{subsection.9.5.1}{}}
\newlabel{ocrf:eq:impurity_measure_decrease_theo}{{9.7}{161}{Underlying model}{equation.9.5.7}{}}
\newlabel{ocrf:eq:gini_theo}{{9.8}{162}{Underlying model}{equation.9.5.8}{}}
\newlabel{ocrf:eq:gini_oc}{{9.9}{162}{Underlying model}{equation.9.5.9}{}}
\newlabel{ocrf:theo_proxy}{{9.10}{162}{Underlying model}{equation.9.5.10}{}}
\newlabel{ocrf:oc_proxy1}{{9.11}{162}{Underlying model}{equation.9.5.11}{}}
\newlabel{ocrf:oc_proxy2}{{9.12}{162}{Underlying model}{equation.9.5.12}{}}
\newlabel{ocrf:def:gamma_t}{{9.13}{163}{Underlying model}{equation.9.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Adaptive approach}{163}{subsection.9.5.2}}
\newlabel{ocrf:oc_proxy_ad1}{{9.14}{163}{Adaptive approach}{equation.9.5.14}{}}
\newlabel{ocrf:constraint1}{{9.15}{163}{Adaptive approach}{equation.9.5.15}{}}
\newlabel{ocrf:constraint2}{{9.16}{164}{Adaptive approach}{equation.9.5.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Conclusion}{164}{section.9.6}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Further details on benchmarks and unsupervised results}{164}{section.9.7}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Results for the unsupervised setting\relax }}{164}{table.caption.63}}
\newlabel{table:results-unsupervised}{{9.3}{164}{Results for the unsupervised setting\relax }{table.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Performances of the algorithms on each dataset in the unsupervised framework: ROC AUCs are on the top, Precision-Recall AUCs in the middle and processing times are displayed below (for each dataset and algorithm). The $x$-axis represents the datasets.\relax }}{165}{figure.caption.64}}
\newlabel{ocrf:figresultsunsupervised}{{9.6}{165}{Performances of the algorithms on each dataset in the unsupervised framework: ROC AUCs are on the top, Precision-Recall AUCs in the middle and processing times are displayed below (for each dataset and algorithm). The $x$-axis represents the datasets.\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces ROC and PR curves for OneClassRF (novelty detection framework)\relax }}{166}{figure.caption.65}}
\newlabel{ocrf:fig:oneclassrf_roc_pr}{{9.7}{166}{ROC and PR curves for OneClassRF (novelty detection framework)\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces ROC and PR curves for OneClassRF (unsupervised framework)\relax }}{166}{figure.caption.66}}
\newlabel{ocrf:fig:oneclassrf_roc_pr_unsupervised}{{9.8}{166}{ROC and PR curves for OneClassRF (unsupervised framework)\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces ROC and PR curves for IsolationForest (novelty detection framework)\relax }}{167}{figure.caption.67}}
\newlabel{ocrf:fig:iforest_roc_pr}{{9.9}{167}{ROC and PR curves for IsolationForest (novelty detection framework)\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces ROC and PR curves for IsolationForest (unsupervised framework)\relax }}{167}{figure.caption.68}}
\newlabel{ocrf:fig:iforest_roc_pr_unsupervised}{{9.10}{167}{ROC and PR curves for IsolationForest (unsupervised framework)\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces ROC and PR curves for OCRFsampling (novelty detection framework)\relax }}{168}{figure.caption.69}}
\newlabel{ocrf:fig:ocrfm_roc_pr}{{9.11}{168}{ROC and PR curves for OCRFsampling (novelty detection framework)\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces ROC and PR curves for OCRFsampling (unsupervised framework)\relax }}{168}{figure.caption.70}}
\newlabel{ocrf:fig:ocrfm_roc_pr_unsupervised}{{9.12}{168}{ROC and PR curves for OCRFsampling (unsupervised framework)\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces ROC and PR curves for OCSVM (novelty detection framework)\relax }}{169}{figure.caption.71}}
\newlabel{ocrf:fig:ocsvm_roc_pr}{{9.13}{169}{ROC and PR curves for OCSVM (novelty detection framework)\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces ROC and PR curves for OCSVM (unsupervised framework)\relax }}{169}{figure.caption.72}}
\newlabel{ocrf:fig:ocsvm_roc_pr_unsupervised}{{9.14}{169}{ROC and PR curves for OCSVM (unsupervised framework)\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces ROC and PR curves for LOF (novelty detection framework)\relax }}{170}{figure.caption.73}}
\newlabel{ocrf:fig:lof_roc_pr}{{9.15}{170}{ROC and PR curves for LOF (novelty detection framework)\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces ROC and PR curves for LOF (unsupervised framework)\relax }}{170}{figure.caption.74}}
\newlabel{ocrf:fig:lof_roc_pr_unsupervised}{{9.16}{170}{ROC and PR curves for LOF (unsupervised framework)\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces ROC and PR curves for Orca (novelty detection framework)\relax }}{171}{figure.caption.75}}
\newlabel{ocrf:fig:orca_roc_pr}{{9.17}{171}{ROC and PR curves for Orca (novelty detection framework)\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces ROC and PR curves for Orca (unsupervised framework)\relax }}{171}{figure.caption.76}}
\newlabel{ocrf:fig:orca_roc_pr_unsupervised}{{9.18}{171}{ROC and PR curves for Orca (unsupervised framework)\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces ROC and PR curves for LSAD (novelty detection framework)\relax }}{172}{figure.caption.77}}
\newlabel{ocrf:fig:LSAnomaly_roc_pr}{{9.19}{172}{ROC and PR curves for LSAD (novelty detection framework)\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces ROC and PR curves for LSAD (unsupervised framework)\relax }}{172}{figure.caption.78}}
\newlabel{ocrf:fig:LSAnomaly_roc_pr_unsupervised}{{9.20}{172}{ROC and PR curves for LSAD (unsupervised framework)\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces ROC and PR curves for RFC (novelty detection framework)\relax }}{173}{figure.caption.79}}
\newlabel{ocrf:fig:rf_roc_pr}{{9.21}{173}{ROC and PR curves for RFC (novelty detection framework)\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces ROC and PR curves for RFC (unsupervised framework)\relax }}{173}{figure.caption.80}}
\newlabel{ocrf:fig:rf_roc_pr_unsupervised}{{9.22}{173}{ROC and PR curves for RFC (unsupervised framework)\relax }{figure.caption.80}{}}
\citation{deHaan1996}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Conclusion, limitations \& perspectives}{175}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:concl}{{10}{175}{Conclusion, limitations \& perspectives}{chapter.10}{}}
\bibstyle{ecs}
\bibdata{Thesis}
\bibcite{Aggarwal2001}{{1}{2001}{{Aggarwal \& Yu}}{{Aggarwal and Yu}}}
\bibcite{Amit1997}{{2}{1997}{{Amit \& Geman}}{{Amit and Geman}}}
\bibcite{Anthony93}{{3}{1993}{{Anthony \& Shawe-Taylor}}{{Anthony and Shawe-Taylor}}}
\bibcite{Baillo2003}{{4}{2003}{{Baillo}}{{}}}
\bibcite{Baillo2001}{{5}{2001}{{Baillo et~al.}}{{Baillo, Cuesta-Albertos, and Cuevas}}}
\bibcite{Barnett94}{{6}{1994}{{Barnett \& Lewis}}{{Barnett and Lewis}}}
\bibcite{Barron2003}{{7}{2003}{{Barron et~al.}}{{Barron, Cardaliaguet, and Jensen}}}
\bibcite{Bay2003}{{8}{2003}{{Bay \& Schwabacher}}{{Bay and Schwabacher}}}
\bibcite{Beirlant2015}{{9}{2016}{{Beirlant et~al.}}{{Beirlant, Escobar-Bach, Goegebeur, and Guillou}}}
\bibcite{Beirlant2004}{{10}{2004}{{Beirlant \& Goegebeur}}{{Beirlant and Goegebeur}}}
\bibcite{BGTS04}{{11}{2006}{{Beirlant et~al.}}{{Beirlant, Goegebeur, Segers, and Teugels}}}
\bibcite{BVT1996}{{12}{1996}{{Beirlant et~al.}}{{Beirlant, Vynckier, and Teugels}}}
\bibcite{Biau2008}{{13}{2008}{{Biau et~al.}}{{Biau, Devroye, and Lugosi}}}
\bibcite{Biau2016}{{14}{2016}{{Biau \& Scornet}}{{Biau and Scornet}}}
\bibcite{Bishop2006}{{15}{2006}{{Bishop}}{{}}}
\bibcite{Blanchard2010}{{16}{2010}{{Blanchard et~al.}}{{Blanchard, Lee, and Scott}}}
\bibcite{Blanchard2004}{{17}{2004}{{Blanchard et~al.}}{{Blanchard, Sch{\"a}fer, and Rozenholc}}}
\bibcite{Bottou2007}{{18}{2007}{{Bottou \& Lin}}{{Bottou and Lin}}}
\bibcite{BBL05}{{19}{2005}{{Boucheron et~al.}}{{Boucheron, Bousquet, and Lugosi}}}
\bibcite{BLM2013}{{20}{2013}{{Boucheron et~al.}}{{Boucheron, Lugosi, and Massart}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{177}{dummy.4}}
\bibcite{Boucheron2012}{{21}{2012}{{Boucheron \& Thomas}}{{Boucheron and Thomas}}}
\bibcite{Boucheron2015}{{22}{2015}{{Boucheron \& Thomas}}{{Boucheron and Thomas}}}
\bibcite{Bousquet04}{{23}{2004}{{Bousquet et~al.}}{{Bousquet, Boucheron, and Lugosi}}}
\bibcite{Breiman2001}{{24}{2001}{{Breiman}}{{}}}
\bibcite{Breunig2000LOF}{{25}{2000}{{Breunig et~al.}}{{Breunig, Kriegel, Ng, and Sander}}}
\bibcite{sklearn_api2013}{{26}{2013}{{Buitinck et~al.}}{{Buitinck, Louppe, Blondel, Pedregosa, Mueller, Grisel, Niculae, Prettenhofer, Gramfort, Grobler, Layton, VanderPlas, Joly, Holt, and Varoquaux}}}
\bibcite{Cadre2006}{{27}{2006}{{Cadre}}{{}}}
\bibcite{Chandola2009survey}{{28}{2009}{{Chandola et~al.}}{{Chandola, Banerjee, and Kumar}}}
\bibcite{Chernozhukov2005}{{29}{2005}{{Chernozhukov}}{{}}}
\bibcite{CLEM13}{{30}{2013}{{Cl\'emen\c {c}on \& Jakubowicz}}{{Cl\'emen\c {c}on and Jakubowicz}}}
\bibcite{CLEM14}{{31}{2014}{{Cl\'emen\c {c}on \& Robbiano}}{{Cl\'emen\c {c}on and Robbiano}}}
\bibcite{Clemencon2009}{{32}{2009{a}}{{Cl{\'e}men{\c {c}}on \& Vayatis}}{{Cl{\'e}men{\c {c}}on and Vayatis}}}
\bibcite{Clemencon2009Tree}{{33}{2009{b}}{{Cl{\'e}men{\c {c}}on \& Vayatis}}{{Cl{\'e}men{\c {c}}on and Vayatis}}}
\bibcite{Clemencon2010}{{34}{2010}{{Cl{\'e}men{\c {c}}on \& Vayatis}}{{Cl{\'e}men{\c {c}}on and Vayatis}}}
\bibcite{Clifton2011}{{35}{2011}{{Clifton et~al.}}{{Clifton, Hugueny, and Tarassenko}}}
\bibcite{Clifton2008}{{36}{2008}{{Clifton et~al.}}{{Clifton, Tarassenko, McGrogan, King, King, and Anuzis}}}
\bibcite{Coles2001}{{37}{2001}{{Coles et~al.}}{{Coles, Bawa, Trenner, and Dorazio}}}
\bibcite{coles1991modeling}{{38}{1991}{{Coles \& Tawn}}{{Coles and Tawn}}}
\bibcite{cooley2010pairwise}{{39}{2010}{{Cooley et~al.}}{{Cooley, Davis, and Naveau}}}
\bibcite{Cortes1995}{{40}{1995}{{Cortes \& Vapnik}}{{Cortes and Vapnik}}}
\bibcite{Cuevas1997}{{41}{1997}{{Cuevas \& Fraiman}}{{Cuevas and Fraiman}}}
\bibcite{Daouia2013}{{42}{2013}{{Daouia et~al.}}{{Daouia, Gardes, and Girard}}}
\bibcite{Daouia2011}{{43}{2011}{{Daouia et~al.}}{{Daouia, Gardes, Girard, and Lekina}}}
\bibcite{Davis2006}{{44}{2006}{{Davis \& Goadrich}}{{Davis and Goadrich}}}
\bibcite{dHF06}{{45}{2007}{{De~Haan \& Ferreira}}{{De~Haan and Ferreira}}}
\bibcite{deHaan1996}{{46}{1996}{{de~Haan \& Resnick}}{{de~Haan and Resnick}}}
\bibcite{dR1977}{{47}{1977}{{de~Haan \& Resnick}}{{de~Haan and Resnick}}}
\bibcite{DEd1989}{{48}{1989}{{Dekkers et~al.}}{{Dekkers, Einmahl, and de~Haan}}}
\bibcite{Denis2005}{{49}{2005}{{Denis et~al.}}{{Denis, Gilleron, and Letouzey}}}
\bibcite{Devroye96}{{50}{1996}{{Devroye et~al.}}{{Devroye, Gy{\"o}rfi, and Lugosi}}}
\bibcite{Diaz2006}{{51}{2006}{{D{\'\i }az-Uriarte \& De~Andres}}{{D{\'\i }az-Uriarte and De~Andres}}}
\bibcite{Drees98}{{52}{1998}{{Drees \& Huang}}{{Drees and Huang}}}
\bibcite{duPlessis2015}{{53}{2015}{{du~Plessis et~al.}}{{du~Plessis, Niu, and Sugiyama}}}
\bibcite{Desir12}{{54}{2012}{{D\IeC {\'e}sir et~al.}}{{D\IeC {\'e}sir, Bernard, Petitjean, and Heutte}}}
\bibcite{Einmahl2006}{{55}{2006}{{Einmahl et~al.}}{{Einmahl, de~Haan, and Li}}}
\bibcite{Einmahl2012}{{56}{2012}{{Einmahl et~al.}}{{Einmahl, Krajina, and Segers}}}
\bibcite{ELL2009}{{57}{2009}{{Einmahl et~al.}}{{Einmahl, Li, and Liu}}}
\bibcite{Einmahl2009}{{58}{2009}{{Einmahl \& Segers}}{{Einmahl and Segers}}}
\bibcite{Einmahl2001}{{59}{2001}{{Einmahl et~al.}}{{Einmahl, de~Haan, and Piterbarg}}}
\bibcite{Einmahl1992}{{60}{1992}{{Einmahl \& Mason}}{{Einmahl and Mason}}}
\bibcite{Embrechts2000}{{61}{2000}{{Embrechts et~al.}}{{Embrechts, de~Haan, and Huang}}}
\bibcite{Eskin2000}{{62}{2000}{{Eskin}}{{}}}
\bibcite{Eskin2002}{{63}{2002}{{Eskin et~al.}}{{Eskin, Arnold, Prerau, Portnoy, and Stolfo}}}
\bibcite{Falk94}{{64}{1994}{{Falk et~al.}}{{Falk, Huesler, and Reiss}}}
\bibcite{Fawcett2006}{{65}{2006}{{Fawcett}}{{}}}
\bibcite{Federer1969}{{66}{1969}{{Federer}}{{}}}
\bibcite{finkenstadt2003extreme}{{67}{2003}{{Finkenstadt \& Rootz{\'e}n}}{{Finkenstadt and Rootz{\'e}n}}}
\bibcite{Flach2003}{{68}{2003}{{Flach}}{{}}}
\bibcite{Fougeres2015}{{69}{2015}{{Fougeres et~al.}}{{Fougeres, De~Haan, and Mercadier}}}
\bibcite{fougeres2009models}{{70}{2009}{{Foug{\`e}res et~al.}}{{Foug{\`e}res, Nolan, and Rootz{\'e}n}}}
\bibcite{Freund1996}{{71}{1996}{{Freund \& Schapire}}{{Freund and Schapire}}}
\bibcite{Friedman2001}{{72}{2001}{{Friedman et~al.}}{{Friedman, Hastie, and Tibshirani}}}
\bibcite{Gardes2008}{{73}{2008}{{Gardes \& Girard}}{{Gardes and Girard}}}
\bibcite{Gardes2010}{{74}{2010}{{Gardes et~al.}}{{Gardes, Girard, and Lekina}}}
\bibcite{Genuer2008}{{75}{2008}{{Genuer et~al.}}{{Genuer, Poggi, and Tuleau}}}
\bibcite{Genuer2010}{{76}{2010}{{Genuer et~al.}}{{Genuer, Poggi, and Tuleau-Malot}}}
\bibcite{Geurts2006}{{77}{2006}{{Geurts et~al.}}{{Geurts, Ernst, and Wehenkel}}}
\bibcite{Gini1912}{{78}{1912}{{Gini}}{{}}}
\bibcite{Girard2004}{{79}{2004}{{Girard}}{{}}}
\bibcite{Girard2008}{{80}{2008}{{Girard \& Jacob}}{{Girard and Jacob}}}
\bibcite{ICMLworkshop16}{{81}{2016}{{Goix}}{{}}}
\bibcite{OCRF16}{{82}{2016{a}}{{Goix et~al.}}{{Goix, Brault, Drougard, and Chiapino}}}
\bibcite{NIPSWORKSHOP15}{{83}{2015{a}}{{Goix et~al.}}{{Goix, Sabourin, and Cl{\'e}men{\c c}on}}}
\bibcite{ARXIV16}{{84}{2016{b}}{{Goix et~al.}}{{Goix, Sabourin, and Cl{\'e}men{\c c}on}}}
\bibcite{COLT15}{{85}{2015{b}}{{Goix et~al.}}{{Goix, Sabourin, and Cl\IeC {\'e}men\IeC {\c c}on}}}
\bibcite{AISTAT15}{{86}{2015{c}}{{Goix et~al.}}{{Goix, Sabourin, and Cl\IeC {\'e}men\IeC {\c c}on}}}
\bibcite{AISTAT16}{{87}{2016{c}}{{Goix et~al.}}{{Goix, Sabourin, and Cl\IeC {\'e}men\IeC {\c c}on}}}
\bibcite{NIPS16evaluation}{{88}{2016}{{Goix \& Thomas}}{{Goix and Thomas}}}
\bibcite{Hartigan1987}{{89}{1987}{{Hartigan}}{{}}}
\bibcite{Hill1975}{{90}{1975}{{Hill}}{{}}}
\bibcite{Ho1998}{{91}{1998}{{Ho}}{{}}}
\bibcite{Hodge2004survey}{{92}{2004}{{Hodge \& Austin}}{{Hodge and Austin}}}
\bibcite{Huangphd}{{93}{1992}{{Huang}}{{}}}
\bibcite{Janson2002}{{94}{2002}{{Janson}}{{}}}
\bibcite{Jones2015scipy}{{95}{2015}{{Jones et~al.}}{{Jones, Oliphant, Peterson, et~al.}}}
\bibcite{KDD99}{{96}{1999}{{KDDCup}}{{}}}
\bibcite{Kolt97}{{97}{1997}{{Koltchinskii}}{{}}}
\bibcite{Kolt06}{{98}{2006}{{Koltchinskii}}{{}}}
\bibcite{Leadbetter1983}{{99}{1983}{{Leadbetter et~al.}}{{Leadbetter, Lindgren, and Rootz{\'e}n}}}
\bibcite{Lee2008}{{100}{2008}{{Lee \& Roberts}}{{Lee and Roberts}}}
\bibcite{Lichman2013}{{101}{2013}{{Lichman}}{{}}}
\bibcite{Lippmann2000}{{102}{2000}{{Lippmann et~al.}}{{Lippmann, Haines, Fried, Korba, and Das}}}
\bibcite{Liu2002}{{103}{2002}{{Liu et~al.}}{{Liu, Lee, Yu, and Li}}}
\bibcite{Liu2012}{{104}{2012}{{Liu et~al.}}{{Liu, Ting, and Zhou}}}
\bibcite{Liu2008}{{105}{2008}{{Liu et~al.}}{{Liu, Ting, and Zhou}}}
\bibcite{Louppe2014}{{106}{2014}{{Louppe}}{{}}}
\bibcite{Markou2003survey}{{107}{2003}{{Markou \& Singh}}{{Markou and Singh}}}
\bibcite{Mason2009}{{108}{2009}{{Mason \& Polonik}}{{Mason and Polonik}}}
\bibcite{Massart2000}{{109}{2000}{{Massart}}{{}}}
\bibcite{Massart2007}{{110}{2007}{{Massart}}{{}}}
\bibcite{McDiarmid98}{{111}{1998}{{McDiarmid}}{{}}}
\bibcite{Mordelet2014}{{112}{2014}{{Mordelet \& Vert}}{{Mordelet and Vert}}}
\bibcite{Muller1991}{{113}{1991}{{M{\"u}ller \& Sawitzki}}{{M{\"u}ller and Sawitzki}}}
\bibcite{Panov2007}{{114}{2007}{{Panov \& D{\v {z}}eroski}}{{Panov and D{\v {z}}eroski}}}
\bibcite{Patcha2007survey}{{115}{2007}{{Patcha \& Park}}{{Patcha and Park}}}
\bibcite{sklearn2011}{{116}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.}}}
\bibcite{Polonik95}{{117}{1995}{{Polonik}}{{}}}
\bibcite{Polonik97}{{118}{1997}{{Polonik}}{{}}}
\bibcite{Polonik1998}{{119}{1998}{{Polonik}}{{}}}
\bibcite{Provost1997}{{120}{1997}{{Provost et~al.}}{{Provost, Fawcett, et~al.}}}
\bibcite{Provost1998}{{121}{1998}{{Provost et~al.}}{{Provost, Fawcett, and Kohavi}}}
\bibcite{Qi97}{{122}{1997}{{Qi}}{{}}}
\bibcite{Quinn2014}{{123}{2014}{{Quinn \& Sugiyama}}{{Quinn and Sugiyama}}}
\bibcite{Resnick1987}{{124}{1987}{{Resnick}}{{}}}
\bibcite{Resnick2007}{{125}{2007}{{Resnick}}{{}}}
\bibcite{Rigollet2009}{{126}{2009}{{Rigollet \& Vert}}{{Rigollet and Vert}}}
\bibcite{Roberts99}{{127}{1999}{{Roberts}}{{}}}
\bibcite{Roberts2000}{{128}{2000}{{Roberts}}{{}}}
\bibcite{sabourinNaveau2012}{{129}{2014}{{Sabourin \& Naveau}}{{Sabourin and Naveau}}}
\bibcite{Scholkopf2001}{{130}{2001}{{Sch{\"o}lkopf et~al.}}{{Sch{\"o}lkopf, Platt, Shawe-Taylor, Smola, and Williamson}}}
\bibcite{Schubert2012}{{131}{2012}{{Schubert et~al.}}{{Schubert, Wojdanowski, Zimek, and Kriegel}}}
\bibcite{Scott2009}{{132}{2009}{{Scott \& Blanchard}}{{Scott and Blanchard}}}
\bibcite{Scott2006}{{133}{2006}{{Scott \& Nowak}}{{Scott and Nowak}}}
\bibcite{Segers12Bernoulli}{{134}{2012{a}}{{Segers}}{{}}}
\bibcite{Segers12}{{135}{2012{b}}{{Segers}}{{}}}
\bibcite{Shannon2001}{{136}{2001}{{Shannon}}{{}}}
\bibcite{Shawe2004}{{137}{2004}{{Shawe-Taylor \& Cristianini}}{{Shawe-Taylor and Cristianini}}}
\bibcite{Shi2012}{{138}{2012}{{Shi \& Horvath}}{{Shi and Horvath}}}
\bibcite{Shyu2003}{{139}{2003}{{Shyu et~al.}}{{Shyu, Chen, Sarinnapakorn, and Chang}}}
\bibcite{Smith1987}{{140}{1987}{{Smith}}{{}}}
\bibcite{smith2003statistics}{{141}{2003}{{Smith}}{{}}}
\bibcite{Smola2009}{{142}{2009}{{Smola et~al.}}{{Smola, Song, and Teo}}}
\bibcite{Steinwart2005}{{143}{2005}{{Steinwart et~al.}}{{Steinwart, Hush, and Scovel}}}
\bibcite{Stephenson2003}{{144}{2003}{{Stephenson}}{{}}}
\bibcite{stephenson2009high}{{145}{2009}{{Stephenson}}{{}}}
\bibcite{Sugiyama2010}{{146}{2010}{{Sugiyama}}{{}}}
\bibcite{Svetnik2003}{{147}{2003}{{Svetnik et~al.}}{{Svetnik, Liaw, Tong, Culberson, Sheridan, and Feuston}}}
\bibcite{Tavallaee2009}{{148}{2009}{{Tavallaee et~al.}}{{Tavallaee, Bagheri, Lu, and Ghorbani}}}
\bibcite{Tawn90}{{149}{1990}{{Tawn}}{{}}}
\bibcite{Tax2002}{{150}{2002}{{Tax \& Duin}}{{Tax and Duin}}}
\bibcite{Thomas2015}{{151}{2015}{{Thomas et~al.}}{{Thomas, Feuillard, and Gramfort}}}
\bibcite{ThomasMaud2015}{{152}{2015}{{Thomas}}{{}}}
\bibcite{Tsybakov1997}{{153}{1997}{{Tsybakov}}{{}}}
\bibcite{Vanderwalt2011numpy}{{154}{2011}{{Van Der~Walt et~al.}}{{Van Der~Walt, Colbert, and Varoquaux}}}
\bibcite{Vapnik2013}{{155}{2013}{{Vapnik}}{{}}}
\bibcite{Vapnik74}{{156}{1974}{{Vapnik \& Chervonenkis}}{{Vapnik and Chervonenkis}}}
\bibcite{VertVert}{{157}{2006}{{Vert \& Vert}}{{Vert and Vert}}}
\bibcite{Vert06thesis}{{158}{2006}{{Vert}}{{}}}
\bibcite{VCTWMS}{{159}{2012}{{Viswanathan et~al.}}{{Viswanathan, Choudur, Talwar, Wang, Macdonald, and Satterfield}}}
\bibcite{Wellner78}{{160}{1978}{{Wellner}}{{}}}
\bibcite{Yamanishi2000}{{161}{2000}{{Yamanishi et~al.}}{{Yamanishi, Takeuchi, Williams, and Milne}}}
